<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
  <title>goonR blog</title>
  <link>/</link>
  <description>Recent content on goonR blog</description>
  <generator>Hugo -- gohugo.io</generator>
<language>en-us</language>
<lastBuildDate>Wed, 28 Nov 2018 00:00:00 +0000</lastBuildDate>

<atom:link href="/index.xml" rel="self" type="application/rss+xml" />


<item>
  <title>Presentation at Delaware Watershed Research Conference 2018</title>
  <link>/2018/11/28/presentation-at-delaware-watershed-research-conference-2018/</link>
  <pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate>
  
<guid>/2018/11/28/presentation-at-delaware-watershed-research-conference-2018/</guid>
  <description>&lt;p&gt;I am pleased to announce that I will presenting at the upcoming &lt;a href=&#34;https://ansp.org/research/environmental-research/projects/watershed-protection-program/delaware-watershed-research-conference/&#34;&gt;Delaware Watershed Reseach Conference 2018&lt;/a&gt; at the Academy of Natural Sciences of Drexel University on Thursday November 29, 2018. My presentation is entitled “Comparing Illumina MiSeq and PacBio Sequel Sequencing of Fecal Samples from Various Animal Sources Potentially Contributing to Microbial Contamination of the Delaware River Watershed”. This presentation will discuss ongoing research within the &lt;a href=&#34;http://microbes.cae.drexel.edu/people/dr-sales&#34;&gt;Sales Laboratory Group&lt;/a&gt; at Drexel University.&lt;/p&gt;
&lt;p&gt;The project is investigating the use of different next generation sequencing techniques and their ability to perform microbial source tracking. The presentation at DWRC 2018 will focus solely on the difference between the two sequencing results for just the fecal samples collected in this study. Specifically, looking at whether sequencing the full length 16S rRNA gene (done using the PacBio Sequel sequencing method) is more effective than targeting specific hyper-variable regions of the 16S rRNA gene (done using the Illumina MiSeq sequencing method). The abstract for this presentation can be found &lt;a href=&#34;https://tbradley1013.github.io/presentations/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I will be presenting at 11 AM in the Freshwater Status and Trends track of the conference.&lt;/p&gt;
&lt;p&gt;Please come and take a look at the research I am working on!&lt;/p&gt;
</description>
  </item>
  
<item>
  <title>Calculating quantiles for groups with dplyr::summarize and purrr::partial</title>
  <link>/2018/10/01/calculating-quantiles-for-groups-with-dplyr-summarize-and-purrr-partial/</link>
  <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
  
<guid>/2018/10/01/calculating-quantiles-for-groups-with-dplyr-summarize-and-purrr-partial/</guid>
  <description>&lt;p&gt;Recently, I was trying to calculate the percentiles of a set of variables within a data set grouped by another variable. However, I quickly ran into the realization that this is not very straight forward when using &lt;code&gt;dplyr&lt;/code&gt;’s &lt;code&gt;summarize&lt;/code&gt;. Before I demonstrate, let’s load the libraries that we will need.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(purrr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you don’t believe me when I say that it is not straight forward, go ahead and try to run the following block of code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;% 
  dplyr::group_by(cyl) %&amp;gt;% 
  dplyr::summarize(quants = quantile(mpg, probs = c(0.2, 0.5, 0.8)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you ran the code, you will see that it throws the following error:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error in summarise_impl(.data, dots) : 
  Column `quants` must be length 1 (a summary value), not 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This error is telling us that the result is returning an object of length 3 (our three quantiles) when it is expecting to get only one value. A quick Google search comes up with &lt;a href=&#34;https://stackoverflow.com/questions/30488389/using-dplyr-window-functions-to-calculate-percentiles&#34;&gt;numerous&lt;/a&gt; &lt;a href=&#34;https://stackoverflow.com/questions/46935464/dplyr-to-count-means-by-group-and-then-quantiles-for-each&#34;&gt;stack&lt;/a&gt; &lt;a href=&#34;https://stackoverflow.com/questions/46177312/how-to-find-quantile-of-grouped-variable-in-dplyr&#34;&gt;overflow&lt;/a&gt; &lt;a href=&#34;https://stackoverflow.com/questions/37845915/how-to-use-quantile-with-dplyr-and-group-by&#34;&gt;questions&lt;/a&gt; and &lt;a href=&#34;https://groups.google.com/forum/#!topic/manipulatr/jEUIbQi-iuA&#34;&gt;answers&lt;/a&gt; about this. Most of these solutions revolve around using the &lt;code&gt;do&lt;/code&gt; function to calculate the quantiles on each of the groups. However, according to &lt;a href=&#34;https://community.rstudio.com/t/should-i-move-away-from-do-and-rowwise/2857/2&#34;&gt;Hadley&lt;/a&gt;, &lt;code&gt;do&lt;/code&gt; will eventually be “going away”. While there is no definite time frame on this, I try to use it as little as possible. The new recommended practice is a combination of &lt;code&gt;tidyr::nest&lt;/code&gt;, &lt;code&gt;dplyr::mutate&lt;/code&gt; and &lt;code&gt;purrr::map&lt;/code&gt; for most cases of grouping. I love this approach for most things (and it is even the accepted for one of &lt;a href=&#34;https://stackoverflow.com/questions/30488389/using-dplyr-window-functions-to-calculate-percentiles&#34;&gt;the SO questions mentioned above&lt;/a&gt;) but I worked up a new solution that I think is useful for calculating percentiles on multiple groups for any desired number of percentiles.&lt;/p&gt;
&lt;p&gt;This method uses &lt;code&gt;purrr::map&lt;/code&gt; and a &lt;a href=&#34;http://adv-r.had.co.nz/Function-operators.html&#34;&gt;Function Operator&lt;/a&gt;, &lt;a href=&#34;https://rdrr.io/cran/purrr/man/partial.html&#34;&gt;&lt;code&gt;purrr::partial&lt;/code&gt;&lt;/a&gt;, to create a list of functions that can than be applied to a data set using &lt;code&gt;dplyr::summarize_at&lt;/code&gt; and a little magic from &lt;code&gt;rlang&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s start by creating a vector of the desired percentiles to calculate. In this example, we will calculate the 20&lt;sup&gt;th&lt;/sup&gt;, 50&lt;sup&gt;th&lt;/sup&gt;, and 80&lt;sup&gt;th&lt;/sup&gt; percentiles.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- c(0.2, 0.5, 0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create a list of functions, with one for each quantile, using &lt;code&gt;purrr::map&lt;/code&gt; and &lt;code&gt;purrr::partial&lt;/code&gt;. We can also assign names to each function (useful for the output of &lt;code&gt;summarize&lt;/code&gt;) using &lt;code&gt;purrr::set_names&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_names &amp;lt;- map_chr(p, ~paste0(.x*100, &amp;quot;%&amp;quot;))

p_funs &amp;lt;- map(p, ~partial(quantile, probs = .x, na.rm = TRUE)) %&amp;gt;% 
  set_names(nm = p_names)

p_funs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $`20%`
## function (...) 
## quantile(probs = .x, na.rm = TRUE, ...)
## &amp;lt;environment: 0x7fcf50757430&amp;gt;
## 
## $`50%`
## function (...) 
## quantile(probs = .x, na.rm = TRUE, ...)
## &amp;lt;environment: 0x7fcf50762c30&amp;gt;
## 
## $`80%`
## function (...) 
## quantile(probs = .x, na.rm = TRUE, ...)
## &amp;lt;environment: 0x7fcf51148830&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at &lt;code&gt;p_funs&lt;/code&gt; we can see that we have a named list with each element containing a function comprised of the &lt;code&gt;quantile&lt;/code&gt; function. The beauty of this is that you can use this list in the same way you would define multiple functions in any other &lt;code&gt;summarize_at&lt;/code&gt; or &lt;code&gt;summarize_all&lt;/code&gt; functions (i.e. &lt;code&gt;funs(mean, sd)&lt;/code&gt;). The only difference is that we will now have to use the “bang-bang-bang” operator (&lt;code&gt;!!!&lt;/code&gt;) from &lt;code&gt;rlang&lt;/code&gt; (it is also exported from &lt;code&gt;dplyr&lt;/code&gt;). The final product looks like this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;% 
  group_by(cyl) %&amp;gt;% 
  summarize_at(vars(mpg), funs(!!!p_funs))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 4
##     cyl `20%` `50%` `80%`
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     4  22.8  26    30.4
## 2     6  18.3  19.7  21  
## 3     8  13.9  15.2  16.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I think that this provides a pretty neat way to get the desired output in a format that does not require a large amount of post calculation manipulation. In addition, it is, in my opinion, more straightforward than a lot of the &lt;code&gt;do&lt;/code&gt; methods. This method also allows for quantiles to be calculated for more than one variable, although post-processing would be necessary in that case. Here is an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;% 
  group_by(cyl) %&amp;gt;% 
  summarize_at(vars(mpg, hp), funs(!!!p_funs)) %&amp;gt;% 
  select(cyl, contains(&amp;quot;mpg&amp;quot;), contains(&amp;quot;hp&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 7
##     cyl `mpg_20%` `mpg_50%` `mpg_80%` `hp_20%` `hp_50%` `hp_80%`
##   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1     4      22.8      26        30.4       65      91        97
## 2     6      18.3      19.7      21        110     110       123
## 3     8      13.9      15.2      16.8      175     192.      245&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;partial&lt;/code&gt; is &lt;em&gt;yet another&lt;/em&gt; tool from the &lt;code&gt;purrr&lt;/code&gt; package that can greatly enhance your R coding abilities. While this is surely a basic application of its functionality, one can easily see how powerful this function can be.&lt;/p&gt;
</description>
  </item>
  
<item>
  <title>Paper Published, Using Historical LCR and Water Quality Data to Evaluate Corrosion Control Treatment in JAWWA</title>
  <link>/2018/09/23/paper-published-using-historical-lcr-and-water-quality-data-to-evaluate-corrosion-control-treatment-in-jawwa/</link>
  <pubDate>Sun, 23 Sep 2018 00:00:00 +0000</pubDate>
  
<guid>/2018/09/23/paper-published-using-historical-lcr-and-water-quality-data-to-evaluate-corrosion-control-treatment-in-jawwa/</guid>
  <description>

&lt;p&gt;I am pleased to announce that my manuscript entitled &amp;ldquo;Using Historical LCR and Water Quality Data to Evaluate Corrosion Control Treatment&amp;rdquo; has been published in the upcoming issue of &lt;a href=&#34;https://awwa.onlinelibrary.wiley.com/journal/15518833&#34;&gt;Journal - American Water Works Association&lt;/a&gt; (&lt;a href=&#34;https://awwa.onlinelibrary.wiley.com/toc/15518833/0/0&#34;&gt;JAWWA October 2018&lt;/a&gt;). This article is a case study that explores the utility of historical LCR and water quality data to assess a drinking water distribution system&amp;rsquo;s corrosion control treatment progress.&lt;/p&gt;

&lt;p&gt;I would like to thank my co-author, Nicola Horscroft, who is the lead Project Engineer for the LCR sampling program for the Philadelphia Water Department (PWD).&lt;/p&gt;

&lt;p&gt;The article can be found from &lt;a href=&#34;https://doi.org/10.1002/awwa.1143&#34;&gt;JAWWA&lt;/a&gt;. This article demonstrates that a drinking water system that is in compliance with the Lead and Copper Rule (LCR) can use historical LCR and water quality data to demonstrate the progress of its Corrosion Control Treatment (CCT). This article explores PWD&amp;rsquo;s historical LCR data to demonstrate how treatment changes, primarily the addition of orthophosphate, have helped to reduce lead levels in the distribution system. In addition this article demonstrates several methods to parse through historical LCR data. One of these methods is using ridge plots to see how the distribution of lead levels has changed over the course of LCR regulatory sampling rounds.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;strong&gt;Distribution of lead results for each LCR sampling round&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/lcr-article-announcement/lcr-ridge-plot.png&#34; alt=&#34;&#34; /&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;citation&#34;&gt;Citation&lt;/h3&gt;

&lt;p&gt;Bradley, Tyler, and Nicola Horscroft. “Using Historical LCR and Water Quality Data to Evaluate Corrosion Control Treatment.” &lt;em&gt;Journal - American Water Works Association&lt;/em&gt;, 21 Sept. 2018, &lt;a href=&#34;https://doi.org/10.1002/awwa.1143&#34;&gt;doi:10.1002/awwa.1143&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;

&lt;p&gt;Historical Lead and Copper Rule (LCR) regulatory sampling data from the Philadelphia Water Department were examined to explore their potential value for guiding a water utility&amp;rsquo;s progress with regard to optimal corrosion control treatment (OCCT). If a system has established a stable water treatment process with consistent corrosion control treatment (CCT) and has achieved continued decreases in lead levels during regulatory sampling, then the information collected during LCR monitoring can be used as an important data set of a broader OCCT evaluation and will help inform the benefit of additional changes in CCT. Since water utilities have LCR data dating back to 1992, these data should be used to make informed decisions. This research also showed that the addition of orthophosphate has resulted in a significant decrease in lead levels at the customer tap. Additionally, profile sampling was performed to show that first‐draw 1 L samples following a 6 h stagnation period provide a good representation of the lead concentrations measured from lead service line and home plumbing samples at the same sites and may be used to indicate overall changes in lead concentrations at the tap resulting from CCT for this system.&lt;/p&gt;
</description>
  </item>
  
<item>
  <title>Tracking Joey Wendle&#39;s rookie season with gganimate</title>
  <link>/2018/09/06/tracking-joey-wendle-rookie-season-with-gganimate/</link>
  <pubDate>Thu, 06 Sep 2018 00:00:00 +0000</pubDate>
  
<guid>/2018/09/06/tracking-joey-wendle-rookie-season-with-gganimate/</guid>
  <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This past weekend I got the chance to go to the Tampa Bay Rays vs Cleveland Indians game. This game was a ton of fun, made even more exciting for me - and by the end of the game, the people in my section - because my brother-in-law’s brother (does that make him my brother-in-law too? No one ever knows for sure…), Joey Wendle, plays for the Rays! In case you haven’t been paying attention, Joey has been having a &lt;strong&gt;&lt;em&gt;MASSIVE&lt;/em&gt;&lt;/strong&gt; rookie season. Just take a look at some of the tweets from the Ray’s organization in the last few days.&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
The AL&#39;s top-hitting rookie is on a record pace.&lt;a href=&#34;https://twitter.com/hashtag/RaysUp?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#RaysUp&lt;/a&gt; &lt;a href=&#34;https://t.co/2oo88IXS2f&#34;&gt;pic.twitter.com/2oo88IXS2f&lt;/a&gt;
&lt;/p&gt;
— Tampa Bay Rays (&lt;span class=&#34;citation&#34;&gt;@RaysBaseball&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/RaysBaseball/status/1037098037120823297?ref_src=twsrc%5Etfw&#34;&gt;September 4, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
How many ways can one man impact a game?&lt;a href=&#34;https://twitter.com/hashtag/RaysUp?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#RaysUp&lt;/a&gt; &lt;a href=&#34;https://t.co/iE3izrtc70&#34;&gt;pic.twitter.com/iE3izrtc70&lt;/a&gt;
&lt;/p&gt;
— Tampa Bay Rays (&lt;span class=&#34;citation&#34;&gt;@RaysBaseball&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/RaysBaseball/status/1036398948502695937?ref_src=twsrc%5Etfw&#34;&gt;September 2, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/center&gt;
&lt;p&gt;As you can see, the man has been getting it done at the plate and in the field. This has led some to call for his inclusion in the AL Rookie of the Year conversation.&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
&lt;a href=&#34;https://twitter.com/hashtag/Rays?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Rays&lt;/a&gt; Joey Wendle put on a show on Sunday and it’s time to start talking about him as a legit AL Rookie of the Year contender: &lt;a href=&#34;https://t.co/G9sFvLLeLq&#34;&gt;https://t.co/G9sFvLLeLq&lt;/a&gt;
&lt;/p&gt;
— Juan Toribio (&lt;span class=&#34;citation&#34;&gt;@juanctoribio&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/juanctoribio/status/1036449226732064773?ref_src=twsrc%5Etfw&#34;&gt;September 3, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/center&gt;
&lt;p&gt;I have been looking for a good excuse to play around with &lt;a href=&#34;https://www.data-imaginist.com/&#34;&gt;Thomas Pedersen’s&lt;/a&gt; &lt;a href=&#34;https://github.com/thomasp85/gganimate&#34;&gt;&lt;code&gt;gganimate&lt;/code&gt;&lt;/a&gt; package, and what better way then by taking a look at Joey’s rookie season.&lt;/p&gt;
&lt;p&gt;Before we start, let’s load the R packages that we will use.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
# devtools::install_github(&amp;quot;BillPetti/baseballr&amp;quot;)
library(baseballr)
# devtools::install_github(&amp;quot;thomasp85/gganimate&amp;quot;)
library(gganimate)
library(lubridate)
library(fuzzyjoin) # for position graph&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Getting the data&lt;/h1&gt;
&lt;p&gt;There are several sources of MLB data available on the internet, and the &lt;a href=&#34;https://github.com/BillPetti/baseballr&#34;&gt;&lt;code&gt;baseballr&lt;/code&gt;&lt;/a&gt; package by &lt;a href=&#34;https://billpetti.github.io/&#34;&gt;Bill Petti&lt;/a&gt; makes getting this data super easy. The sources of data that I will use in this post are from the &lt;a href=&#34;http://baseballsavant.mlb.com&#34;&gt;Baseball Savant&lt;/a&gt; website for batting data and &lt;a href=&#34;https://www.FanGraphs.com&#34;&gt;Fan Graphs&lt;/a&gt; for fielding position. These sites both provide game by game data for any player in the league for any season. The Baseball Savant site provides pitch by pitch data about each of Joey’s at-bats and the Fan Graphs site provides game by game stats.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;baseballr&lt;/code&gt; provides different functions for the different data sources, but first we will have to get Joey’s player id from these sites. We can use the &lt;code&gt;playerid_lookup&lt;/code&gt; function to find it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;playerid_lookup(&amp;quot;Wendle&amp;quot;, &amp;quot;Joey&amp;quot;) %&amp;gt;% glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 1
## Variables: 11
## $ first_name       &amp;lt;chr&amp;gt; &amp;quot;Joey&amp;quot;
## $ last_name        &amp;lt;chr&amp;gt; &amp;quot;Wendle&amp;quot;
## $ given_name       &amp;lt;chr&amp;gt; &amp;quot;Joseph Patrick&amp;quot;
## $ name_suffix      &amp;lt;chr&amp;gt; NA
## $ nick_name        &amp;lt;chr&amp;gt; NA
## $ birth_year       &amp;lt;int&amp;gt; 1990
## $ mlb_played_first &amp;lt;int&amp;gt; 2016
## $ mlbam_id         &amp;lt;int&amp;gt; 621563
## $ retrosheet_id    &amp;lt;chr&amp;gt; &amp;quot;wendj002&amp;quot;
## $ bbref_id         &amp;lt;chr&amp;gt; &amp;quot;wendljo01&amp;quot;
## $ fangraphs_id     &amp;lt;int&amp;gt; 13853&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can look up both the Fan Graphs and Baseball Savant data. From the above we can see that his Fan Graphs id is “13853”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;joey_fg &amp;lt;- baseballr::batter_game_logs_fg(&amp;quot;13853&amp;quot;, year = 2018)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;joey_fg %&amp;gt;% as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 119 x 23
##    Date  Team  Opp   BO    Pos   PA    H     X2B   X3B   HR    R     RBI  
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
##  1 2018~ TBR   @TOR  4     2B    4     0     0     0     0     0     0    
##  2 2018~ TBR   @TOR  5     2B    4     0     0     0     0     0     0    
##  3 2018~ TBR   @TOR  5     SS-3B 4     3     1     0     0     1     0    
##  4 2018~ TBR   @CLE  1     3B    5     2     1     0     0     1     1    
##  5 2018~ TBR   @CLE  1     2B    5     2     1     0     0     1     0    
##  6 2018~ TBR   @CLE  1     2B    4     1     0     0     0     0     0    
##  7 2018~ TBR   @ATL  5     2B    5     2     0     0     0     2     2    
##  8 2018~ TBR   @ATL  1     2B    5     2     0     0     0     1     0    
##  9 2018~ TBR   BOS   1     2B    5     2     1     0     0     1     0    
## 10 2018~ TBR   BOS   1     2B    5     2     0     0     0     0     1    
## # ... with 109 more rows, and 11 more variables: SB &amp;lt;chr&amp;gt;, CS &amp;lt;chr&amp;gt;,
## #   BB_perc &amp;lt;dbl&amp;gt;, K_perc &amp;lt;dbl&amp;gt;, ISO &amp;lt;chr&amp;gt;, BABIP &amp;lt;chr&amp;gt;, AVG &amp;lt;chr&amp;gt;,
## #   OBP &amp;lt;chr&amp;gt;, SLG &amp;lt;chr&amp;gt;, wOBA &amp;lt;chr&amp;gt;, wRC_plus &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can do the same with the Baseball Savant data using the corresponding player id - “621563”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;joey_bsvnt &amp;lt;- scrape_statcast_savant_batter(start_date = &amp;quot;2018-03-29&amp;quot;, end_date = Sys.Date(), batterid = &amp;quot;621563&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;joey_bsvnt %&amp;gt;% as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,687 x 90
##    pitch_type game_date  release_speed release_pos_x release_pos_z
##    &amp;lt;chr&amp;gt;      &amp;lt;date&amp;gt;             &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;
##  1 SI         2018-09-05          92.5         -2.44          5.48
##  2 SI         2018-09-05          93.2         -2.39          5.70
##  3 CH         2018-09-05          88.4         -2.03          5.53
##  4 CU         2018-09-05          78.5         -1.87          5.67
##  5 CU         2018-09-05          80.6         -1.98          5.63
##  6 CU         2018-09-05          79.3         -1.92          5.65
##  7 CU         2018-09-05          78.1         -2.02          5.69
##  8 FT         2018-09-05          92.3         -1.90          5.69
##  9 CH         2018-09-05          89.3         -1.92          5.57
## 10 FT         2018-09-05          93.9         -1.88          5.60
## # ... with 1,677 more rows, and 85 more variables: player_name &amp;lt;chr&amp;gt;,
## #   batter &amp;lt;int&amp;gt;, pitcher &amp;lt;int&amp;gt;, events &amp;lt;chr&amp;gt;, description &amp;lt;chr&amp;gt;,
## #   spin_dir &amp;lt;chr&amp;gt;, spin_rate_deprecated &amp;lt;chr&amp;gt;,
## #   break_angle_deprecated &amp;lt;chr&amp;gt;, break_length_deprecated &amp;lt;chr&amp;gt;,
## #   zone &amp;lt;int&amp;gt;, des &amp;lt;chr&amp;gt;, game_type &amp;lt;chr&amp;gt;, stand &amp;lt;chr&amp;gt;, p_throws &amp;lt;chr&amp;gt;,
## #   home_team &amp;lt;chr&amp;gt;, away_team &amp;lt;chr&amp;gt;, type &amp;lt;chr&amp;gt;, hit_location &amp;lt;int&amp;gt;,
## #   bb_type &amp;lt;chr&amp;gt;, balls &amp;lt;int&amp;gt;, strikes &amp;lt;int&amp;gt;, game_year &amp;lt;int&amp;gt;,
## #   pfx_x &amp;lt;dbl&amp;gt;, pfx_z &amp;lt;dbl&amp;gt;, plate_x &amp;lt;dbl&amp;gt;, plate_z &amp;lt;dbl&amp;gt;, on_3b &amp;lt;dbl&amp;gt;,
## #   on_2b &amp;lt;dbl&amp;gt;, on_1b &amp;lt;dbl&amp;gt;, outs_when_up &amp;lt;int&amp;gt;, inning &amp;lt;int&amp;gt;,
## #   inning_topbot &amp;lt;chr&amp;gt;, hc_x &amp;lt;dbl&amp;gt;, hc_y &amp;lt;dbl&amp;gt;, tfs_deprecated &amp;lt;chr&amp;gt;,
## #   tfs_zulu_deprecated &amp;lt;chr&amp;gt;, pos2_person_id &amp;lt;int&amp;gt;, umpire &amp;lt;chr&amp;gt;,
## #   sv_id &amp;lt;chr&amp;gt;, vx0 &amp;lt;dbl&amp;gt;, vy0 &amp;lt;dbl&amp;gt;, vz0 &amp;lt;dbl&amp;gt;, ax &amp;lt;dbl&amp;gt;, ay &amp;lt;dbl&amp;gt;,
## #   az &amp;lt;dbl&amp;gt;, sz_top &amp;lt;dbl&amp;gt;, sz_bot &amp;lt;dbl&amp;gt;, hit_distance_sc &amp;lt;int&amp;gt;,
## #   launch_speed &amp;lt;dbl&amp;gt;, launch_angle &amp;lt;dbl&amp;gt;, effective_speed &amp;lt;dbl&amp;gt;,
## #   release_spin_rate &amp;lt;int&amp;gt;, release_extension &amp;lt;dbl&amp;gt;, game_pk &amp;lt;int&amp;gt;,
## #   pos1_person_id &amp;lt;int&amp;gt;, pos2_person_id_1 &amp;lt;int&amp;gt;, pos3_person_id &amp;lt;int&amp;gt;,
## #   pos4_person_id &amp;lt;int&amp;gt;, pos5_person_id &amp;lt;int&amp;gt;, pos6_person_id &amp;lt;int&amp;gt;,
## #   pos7_person_id &amp;lt;int&amp;gt;, pos8_person_id &amp;lt;int&amp;gt;, pos9_person_id &amp;lt;int&amp;gt;,
## #   release_pos_y &amp;lt;dbl&amp;gt;, estimated_ba_using_speedangle &amp;lt;dbl&amp;gt;,
## #   estimated_woba_using_speedangle &amp;lt;dbl&amp;gt;, woba_value &amp;lt;dbl&amp;gt;,
## #   woba_denom &amp;lt;int&amp;gt;, babip_value &amp;lt;int&amp;gt;, iso_value &amp;lt;int&amp;gt;,
## #   launch_speed_angle &amp;lt;int&amp;gt;, at_bat_number &amp;lt;int&amp;gt;, pitch_number &amp;lt;int&amp;gt;,
## #   pitch_name &amp;lt;chr&amp;gt;, home_score &amp;lt;int&amp;gt;, away_score &amp;lt;int&amp;gt;, bat_score &amp;lt;int&amp;gt;,
## #   fld_score &amp;lt;int&amp;gt;, post_away_score &amp;lt;int&amp;gt;, post_home_score &amp;lt;int&amp;gt;,
## #   post_bat_score &amp;lt;int&amp;gt;, post_fld_score &amp;lt;int&amp;gt;,
## #   if_fielding_alignment &amp;lt;chr&amp;gt;, of_fielding_alignment &amp;lt;chr&amp;gt;, barrel &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While the baseball savant data has a ton of information, it does not have stats per game or cumulative stats for the season. However, using the data that we have, these things can be calculated easily! To do this, we will define a custom function to calculate a few standard sabermetrics (i.e. batting average, slugging, on base percentage, etc.).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;savant_daily &amp;lt;- function(data){
  output &amp;lt;- data %&amp;gt;% 
    filter(!is.na(events), events != &amp;quot;caught_stealing_2b&amp;quot;) %&amp;gt;% 
    mutate(
      is_ab = if_else(
        events %in% c(&amp;quot;strikeout&amp;quot;, &amp;quot;field_out&amp;quot;, &amp;quot;single&amp;quot;, &amp;quot;double&amp;quot;, &amp;quot;force_out&amp;quot;, 
                      &amp;quot;triple&amp;quot;, &amp;quot;home_run&amp;quot;, &amp;quot;double_play&amp;quot;, &amp;quot;field_error&amp;quot;, &amp;quot;grounded_into_double_play&amp;quot;,
                      &amp;quot;strikeout_double_play&amp;quot;, &amp;quot;fielders_choice_out&amp;quot;),
        TRUE,
        FALSE
      ),
      is_hit = if_else(
        events %in% c(&amp;quot;single&amp;quot;, &amp;quot;double&amp;quot;, &amp;quot;triple&amp;quot;, &amp;quot;home_run&amp;quot;), TRUE, FALSE
      ),
      bases = case_when(
        events == &amp;quot;single&amp;quot; ~ 1,
        events == &amp;quot;double&amp;quot; ~ 2,
        events == &amp;quot;triple&amp;quot; ~ 3,
        events == &amp;quot;home_run&amp;quot; ~ 4,
        TRUE ~ 0
      ),
      event_custom = case_when(
        events %in% c(&amp;quot;single&amp;quot;, &amp;quot;double&amp;quot;, &amp;quot;triple&amp;quot;, &amp;quot;home_run&amp;quot;) ~ events, 
        str_detect(events, &amp;quot;sac&amp;quot;) ~ &amp;quot;sacrifice&amp;quot;,
        events %in% c(&amp;quot;walk&amp;quot;, &amp;quot;hit_by_pitch&amp;quot;) ~ NA_character_,
        events == &amp;quot;field_error&amp;quot; ~ &amp;quot;error&amp;quot;,
        TRUE ~ &amp;quot;out&amp;quot;
      )
    ) %&amp;gt;% 
    group_by(game_date) %&amp;gt;% 
    summarize(
      pa = length(unique(at_bat_number)),
      ab = sum(is_ab),
      hits = sum(is_hit),
      doubles = sum(events == &amp;quot;double&amp;quot;),
      triples = sum(events == &amp;quot;triples&amp;quot;),
      home_runs = sum(events == &amp;quot;home_run&amp;quot;),
      bb = sum(events == &amp;quot;walk&amp;quot;),
      hbp = sum(events == &amp;quot;hit_by_pitch&amp;quot;),
      so = sum(events %in% c(&amp;quot;strikeout&amp;quot;, &amp;quot;strikeout_double_play&amp;quot;)),
      bases = sum(bases)
    ) %&amp;gt;% 
    arrange(game_date) %&amp;gt;% 
    mutate(
      ba = round(hits/ab, 3),
      obp = round((hits + bb + hbp)/(ab + bb + hbp), 3),
      slg = round(bases/ab, 3),
      ops = obp + slg,
      hits_to_date = cumsum(hits),
      bb_to_date = cumsum(bb),
      hbp_to_date = cumsum(hbp),
      ab_to_date = cumsum(ab),
      bases_to_date = cumsum(bases),
      ba_to_date = round(hits_to_date/ab_to_date, 3),
      obp_to_date = round(
        (hits_to_date + bb_to_date + hbp_to_date)/(ab_to_date + bb_to_date + hbp_to_date), 3
      ),
      slg_to_date = round(bases_to_date/ab_to_date, 3),
      ops_to_date = obp_to_date + slg_to_date
    )
  
  return(output)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;joey_bsvnt_daily &amp;lt;- savant_daily(joey_bsvnt)

joey_bsvnt_daily %&amp;gt;% glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 117
## Variables: 24
## $ game_date     &amp;lt;date&amp;gt; 2018-03-29, 2018-03-30, 2018-03-31, 2018-04-01,...
## $ pa            &amp;lt;int&amp;gt; 1, 1, 4, 4, 3, 1, 3, 2, 5, 6, 4, 4, 4, 4, 4, 2, ...
## $ ab            &amp;lt;int&amp;gt; 1, 1, 3, 4, 3, 1, 3, 2, 5, 3, 3, 4, 3, 2, 3, 2, ...
## $ hits          &amp;lt;int&amp;gt; 0, 0, 0, 2, 0, 1, 2, 1, 2, 1, 0, 0, 1, 0, 0, 1, ...
## $ doubles       &amp;lt;int&amp;gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...
## $ triples       &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
## $ home_runs     &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...
## $ bb            &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 1, 1, 1, 0, ...
## $ hbp           &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...
## $ so            &amp;lt;int&amp;gt; 0, 0, 2, 0, 0, 0, 1, 0, 1, 2, 1, 2, 0, 1, 0, 1, ...
## $ bases         &amp;lt;dbl&amp;gt; 0, 0, 0, 3, 0, 1, 3, 3, 5, 1, 0, 0, 2, 0, 0, 1, ...
## $ ba            &amp;lt;dbl&amp;gt; 0.000, 0.000, 0.000, 0.500, 0.000, 1.000, 0.667,...
## $ obp           &amp;lt;dbl&amp;gt; 0.000, 0.000, 0.000, 0.500, 0.000, 1.000, 0.667,...
## $ slg           &amp;lt;dbl&amp;gt; 0.000, 0.000, 0.000, 0.750, 0.000, 1.000, 1.000,...
## $ ops           &amp;lt;dbl&amp;gt; 0.000, 0.000, 0.000, 1.250, 0.000, 2.000, 1.667,...
## $ hits_to_date  &amp;lt;int&amp;gt; 0, 0, 0, 2, 2, 3, 5, 6, 8, 9, 9, 9, 10, 10, 10, ...
## $ bb_to_date    &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 5, 6, 7, 7, ...
## $ hbp_to_date   &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...
## $ ab_to_date    &amp;lt;int&amp;gt; 1, 2, 5, 9, 12, 13, 16, 18, 23, 26, 29, 33, 36, ...
## $ bases_to_date &amp;lt;dbl&amp;gt; 0, 0, 0, 3, 3, 4, 7, 10, 15, 16, 16, 16, 18, 18,...
## $ ba_to_date    &amp;lt;dbl&amp;gt; 0.000, 0.000, 0.000, 0.222, 0.167, 0.231, 0.312,...
## $ obp_to_date   &amp;lt;dbl&amp;gt; 0.000, 0.000, 0.000, 0.222, 0.167, 0.231, 0.312,...
## $ slg_to_date   &amp;lt;dbl&amp;gt; 0.000, 0.000, 0.000, 0.333, 0.250, 0.308, 0.438,...
## $ ops_to_date   &amp;lt;dbl&amp;gt; 0.000, 0.000, 0.000, 0.555, 0.417, 0.539, 0.750,...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fielding-positions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fielding Positions&lt;/h1&gt;
&lt;p&gt;Joey started the season sharing second base duties and was not used in many other positions. However, as the season has progressed, the Rays have given him opportunities at several other positions and Joey has stepped up every time. I mean, just look at those web gems in the tweet at the beginning of this post! Seriously, go watch them again, I will wait…&lt;/p&gt;
&lt;p&gt;As a first stab at &lt;code&gt;gganimate&lt;/code&gt;, let’s take a look at where Joey has played each game with the help of &lt;code&gt;baseballr&lt;/code&gt;’s &lt;code&gt;ggspraychart&lt;/code&gt; function. First, we can define coordinates (relative to &lt;code&gt;ggspraycharts&lt;/code&gt; field definition) for each of the positions that he has played.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;position_loc &amp;lt;- tribble(
  ~position, ~x, ~y, 
  &amp;quot;2B&amp;quot;, 145, -145,
  &amp;quot;3B&amp;quot;, 100, -160,
  &amp;quot;SS&amp;quot;, 111, -144,
  &amp;quot;LF&amp;quot;, 80, -100,
  &amp;quot;RF&amp;quot;, 176, -100
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since Joey has played multiple positions in single games, a simple left join is not entirely appropriate. Using the &lt;a href=&#34;https://github.com/dgrtwo/fuzzyjoin&#34;&gt;&lt;code&gt;fuzzyjoin&lt;/code&gt;&lt;/a&gt; package, we can match the position coordinates with the positions that are defined in the &lt;code&gt;joey_fg&lt;/code&gt; data set by regex.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;joey_position &amp;lt;- joey_fg %&amp;gt;% 
  fuzzyjoin::regex_left_join(position_loc, by = c(&amp;quot;Pos&amp;quot; = &amp;quot;position&amp;quot;)) %&amp;gt;% 
  mutate(Date = lubridate::ymd(Date))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, using the &lt;code&gt;ggspraychart&lt;/code&gt; and the &lt;code&gt;transition_time&lt;/code&gt; function, we can see how his position has changed with each day.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;position_chart &amp;lt;- joey_position %&amp;gt;% 
  filter(!is.na(position)) %&amp;gt;% 
  ggspraychart(x_value = &amp;quot;x&amp;quot;, y_value = &amp;quot;y&amp;quot;, fill_value = &amp;quot;position&amp;quot;) + 
  scale_fill_manual(values = c(&amp;quot;2B&amp;quot; = &amp;quot;red&amp;quot;, &amp;quot;3B&amp;quot; = &amp;quot;blue&amp;quot;,
                               &amp;quot;SS&amp;quot; = &amp;quot;green&amp;quot;, &amp;quot;LF&amp;quot; = &amp;quot;yellow&amp;quot;, 
                               &amp;quot;RF&amp;quot; = &amp;quot;black&amp;quot;), na.translate = FALSE) + 
  labs(title = &amp;quot;Joey Wendle 2018 position by game:{frame_time}&amp;quot;,
       caption = &amp;quot;Data source: fangraphs.com\nBuilt with the baseballr package\n&amp;quot;) + 
  transition_time(Date) + 
  theme(
    legend.title = element_blank(),
    plot.title = element_text(size = 14),
    plot.caption = element_text(size = 10, face = &amp;quot;bold&amp;quot;),
    legend.text = element_text(size = 12, face = &amp;quot;bold&amp;quot;)
  )

animate(position_chart, width = 800, height = 800)&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;img src=&#34;/images/joey-rookie/position-chart.gif&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;As this animation shows, he has been moved around the field much more towards the end of the year!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;batting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Batting&lt;/h1&gt;
&lt;p&gt;Now let’s use the &lt;code&gt;ggspraychart&lt;/code&gt; to see how Joey’s hits have accumulated over the course of the season and where he tends to hit the ball. For this, we will use the raw baseball savant data set &lt;code&gt;joey_bsvnt&lt;/code&gt; and use the function &lt;code&gt;transition_reveal&lt;/code&gt;. This function allows for data to be added over a range rather than only showing the data for a specific date.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;joey_spray &amp;lt;- joey_bsvnt %&amp;gt;% 
  filter(events %in% c(&amp;quot;single&amp;quot;, &amp;quot;double&amp;quot;, &amp;quot;triple&amp;quot;, &amp;quot;home_run&amp;quot;)) %&amp;gt;% 
  mutate(events = factor(events, levels = c(&amp;quot;single&amp;quot;, &amp;quot;double&amp;quot;, &amp;quot;triple&amp;quot;, &amp;quot;home_run&amp;quot;),
                         labels = c(&amp;quot;Single&amp;quot;, &amp;quot;Double&amp;quot;, &amp;quot;Triple&amp;quot;, &amp;quot;Home Run&amp;quot;))) %&amp;gt;% 
  ggspraychart(fill_value = &amp;quot;events&amp;quot;, 
               fill_palette = c(&amp;quot;Single&amp;quot;=&amp;quot;#A2C8EC&amp;quot;, &amp;quot;Double&amp;quot;=&amp;quot;#006BA4&amp;quot;, 
                                &amp;quot;Triple&amp;quot;=&amp;quot;#FF940E&amp;quot;, &amp;quot;Home Run&amp;quot;=&amp;quot;#C85200&amp;quot;)) + 
  labs(title = &amp;quot;Joey Wendle 2018 hits to date: {frame_time}&amp;quot;,
       caption = &amp;quot;Data source: baseballsavant.com\nBuilt with the baseballr package\n&amp;quot;) + 
  transition_reveal(game_date, game_date) + 
  theme(
    legend.title = element_blank(),
    plot.title = element_text(size = 14),
    plot.caption = element_text(size = 10, face = &amp;quot;bold&amp;quot;),
    legend.text = element_text(size = 12, face = &amp;quot;bold&amp;quot;)
  )


animate(joey_spray, width = 800, height = 800)&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;img src=&#34;/images/joey-rookie/spray-anim.gif&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;From this animation, Joey seems to hit most of his home runs to right field, but other than that he seems to spread his hits all around the field.&lt;/p&gt;
&lt;p&gt;Let’s take a look at how Joey’s batting average has changed over time. We can use the daily statistics we calculated earlier to see how it changes over time using the &lt;code&gt;transition_reveal&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;joey_rbi &amp;lt;- joey_bsvnt_daily %&amp;gt;% 
  ggplot(aes(game_date, ba_to_date)) + 
  geom_step(size = 1.5) + 
  theme_bw() + 
  labs(
    y = &amp;quot;Batting Average&amp;quot;,
    title = &amp;quot;Joey Wendle&amp;#39;s batting average during the 2018 season&amp;quot;,
    caption = &amp;quot;Data source: baseballsavant.com\n&amp;quot;
  ) + 
  scale_x_date(date_breaks = &amp;quot;2 weeks&amp;quot;, date_labels = &amp;quot;%B %d, %Y&amp;quot;) + 
  scale_y_continuous(breaks = seq(0, 0.35, 0.05)) + 
  theme(
    plot.title = element_text(size = 14, face = &amp;quot;bold&amp;quot;),
    plot.caption = element_text(size = 10, face = &amp;quot;bold&amp;quot;),
    axis.title.x = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, face = &amp;quot;bold&amp;quot;),
    axis.text.y = element_text(face = &amp;quot;bold&amp;quot;)
  ) + 
  transition_reveal(id = game_date, along = game_date) 


animate(joey_rbi, width = 800, height = 600)&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;img src=&#34;/images/joey-rookie/rbi-anim.gif&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;As we can see, Joey has stayed fairly consistent between 0.275 and 0.300 and has only been improving since the all-star break. Using the same functionality, we can look at multiple stats all at once by simply faceting the graphs. Here we can see how his batting average, slugging percentage and on-base percentage have changed over the course of the season.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;multi_stats &amp;lt;- joey_bsvnt_daily %&amp;gt;% 
  select(game_date, ba_to_date, slg_to_date, obp_to_date) %&amp;gt;% 
  gather(key = &amp;quot;metric&amp;quot;, value = &amp;quot;value&amp;quot;, -game_date) %&amp;gt;% 
  mutate(metric = case_when(
    metric == &amp;quot;ba_to_date&amp;quot; ~ &amp;quot;Batting Average&amp;quot;,
    metric == &amp;quot;slg_to_date&amp;quot; ~ &amp;quot;Slugging&amp;quot;,
    metric == &amp;quot;obp_to_date&amp;quot; ~ &amp;quot;On-base Percentage&amp;quot;
  )) %&amp;gt;% 
  ggplot(aes(game_date, value)) + 
  geom_step(size = 1.5) + 
  facet_wrap(vars(metric), scales = &amp;quot;free_y&amp;quot;, ncol = 1) + 
  theme_bw() + 
  scale_x_date(date_breaks = &amp;quot;2 week&amp;quot;, date_labels = &amp;quot;%B %d, %Y&amp;quot;) + 
  theme(
    plot.title = element_text(size = 14, face = &amp;quot;bold&amp;quot;),
    plot.caption = element_text(size = 10, face = &amp;quot;bold&amp;quot;),
    axis.title = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, face = &amp;quot;bold&amp;quot;),
    axis.text.y = element_text(face = &amp;quot;bold&amp;quot;)
  ) + 
  labs(
   title = &amp;quot;Joey Wendle&amp;#39;s batting average, slugging, and on-base percentage, respectively&amp;quot;,
   caption = &amp;quot;Data source: baseballsavant.com\n&amp;quot;
  ) + 
  transition_reveal(game_date, game_date)

animate(multi_stats, width = 800, height = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;img src=&#34;/images/joey-rookie/multi-stats.gif&#34; /&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Joey is having a breakout year and the &lt;code&gt;gganimate&lt;/code&gt; package gives us a cool way of viewing his progress over the course of the season. I hope you enjoyed the examples and I hope you root for Joey to finish off the year strong!&lt;/p&gt;
&lt;p&gt;Please let me know what you think!&lt;/p&gt;
&lt;/div&gt;
</description>
  </item>
  
<item>
  <title>Add RStudio Community to your blogs social links - Blackburn Theme</title>
  <link>/2018/08/24/add-rstudio-community-to-your-blogs-social-links/</link>
  <pubDate>Fri, 24 Aug 2018 00:00:00 +0000</pubDate>
  
<guid>/2018/08/24/add-rstudio-community-to-your-blogs-social-links/</guid>
  <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Recently, I switched themes for my blog to the &lt;a href=&#34;https://github.com/yoshiharuyamashita/blackburn&#34;&gt;blackburn&lt;/a&gt; theme and I love it! Luckily, the &lt;a href=&#34;https://github.com/rstudio/blogdown&#34;&gt;&lt;code&gt;blogdown&lt;/code&gt;&lt;/a&gt; package makes this super easy to do within R! If you haven’t used &lt;code&gt;blogdown&lt;/code&gt;, I &lt;strong&gt;&lt;em&gt;highly&lt;/em&gt;&lt;/strong&gt; recommend you take a look at it for setting up your blog. One of the nice features of the blackburn theme, is that it provides easy to add “social” tags that can be configured in your &lt;code&gt;config.toml&lt;/code&gt; file. The built in social tags all come with nice &lt;a href=&#34;https://fontawesome.com/&#34;&gt;font-awesome&lt;/a&gt; icons to their left.&lt;/p&gt;
&lt;p&gt;However, one of the social media sites that I visit the most is not included on the provided sites, and that is &lt;a href=&#34;https://community.rstudio.com/&#34;&gt;RStudio Community&lt;/a&gt;! RStudio Community is a web-forum that is hosted by RStudio to provide community Q and A for all things related to R, RStudio products, and packages provided by RStudio. If you haven’t been there before, I highly recommend that you check it out! Since this site is not included in blackburn’s social links, I had to add it myself. This post will walk through how to edit the blackburn theme to include custom social links, specifically for RStudio Community.&lt;/p&gt;
&lt;p&gt;This post assumes that you have already successfully set up your blog using the &lt;code&gt;blackburn&lt;/code&gt; theme. If not, you should check out a few of these links:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;Blogdown book&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://themes.gohugo.io/blackburn/&#34;&gt;Blackburn Theme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mltconsecol.github.io/post/20170123_blogdown_hugo/&#34;&gt;Blog post using blogdown setup with blackburn theme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://tclavelle.github.io/blog/blogdown_github/&#34;&gt;Blog post about blogdown setup in general&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-the-icon&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Creating the icon&lt;/h1&gt;
&lt;p&gt;As mentioned above, all of the icons included next to the default social links are added via font-awesome. Unfortunately, neither RStudio or RStudio Community have a font awesome icon, as of this writing. So before we add the link to our social list, we will have to create our own icon (or in this case, simply an image) for this item. This can be done with a little help from the &lt;a href=&#34;https://github.com/ropensci/magick&#34;&gt;&lt;code&gt;magick&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;The image we are going to use is the same one used as the favicon when you open RStudio Community. If you don’t know what a favicon is, it is simply the image that you see on the left hand side of a browser tab. To get this image, we can use the &lt;code&gt;image_read&lt;/code&gt; function. Before we do that, let’s load the packages that&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)
library(magick)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To load the image we can use the following command&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;community_icon_orig &amp;lt;- image_read(&amp;quot;https://www.rstudio.com/wp-content/uploads/2018/03/community-favicon-512.png&amp;quot;)

community_icon_orig&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-23-add-rstudio-community-to-your-blogs-social-links_files/figure-html/load-image-1.png&#34; width=&#34;256&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Right now the image looks great, but we want our icon to be gray scaled to match the other icons on the social link. We can do this with just a little &lt;code&gt;magick&lt;/code&gt;! First, we will create a background icon that is simply the RStudio Community icon shape in all black. This is because the inner portion of the other social icons is a black color (&lt;code&gt;#191818&lt;/code&gt; to be exact) and the community icon is white when we first load it. Using &lt;code&gt;image_composite&lt;/code&gt;, in conjunction with &lt;code&gt;image_channel&lt;/code&gt; (to convert to gray scale) and &lt;code&gt;image_transparent&lt;/code&gt; (to remove the square background created by &lt;code&gt;image_channel&lt;/code&gt;) we can superimpose the black background over the white portions of the icon. You can create the background for the icon like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;icon_background &amp;lt;- community_icon_orig %&amp;gt;% 
  image_channel(channel = &amp;quot;Gray&amp;quot;) %&amp;gt;% 
  image_transparent(color = &amp;quot;black&amp;quot;) %&amp;gt;% 
  image_composite(image_blank(512, 512, &amp;quot;#191818&amp;quot;), &amp;quot;In&amp;quot;)

icon_background&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-23-add-rstudio-community-to-your-blogs-social-links_files/figure-html/icon-background-1.png&#34; width=&#34;256&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that we have the background to use it in our &lt;code&gt;image_composite&lt;/code&gt; function to create our final community icon with a gray outer portion and a black &lt;code&gt;R&lt;/code&gt; in the center. This is what the final icon will look like!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;community_icon &amp;lt;- community_icon_orig %&amp;gt;% 
  image_channel(channel = &amp;quot;Gray&amp;quot;) %&amp;gt;% 
  image_transparent(color = &amp;quot;black&amp;quot;) %&amp;gt;% 
  image_composite(icon_background, &amp;quot;Subtract&amp;quot;) 

community_icon&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-08-23-add-rstudio-community-to-your-blogs-social-links_files/figure-html/manipulate-image-1.png&#34; width=&#34;256&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we just need to save our image to our blogdown repository. This image should be saved in the &lt;code&gt;static/images/&lt;/code&gt; folder of your blogdown repository. Saving it in this location will allow for us to add it to the blogs menu from any location. We will also scale our image to a width and height of 14px&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;community_icon %&amp;gt;% 
  image_scale(&amp;quot;14&amp;quot;) %&amp;gt;% 
  image_write(&amp;quot;static/images/community-icon.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-the-icon-to-social-list&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Adding the Icon to Social List&lt;/h1&gt;
&lt;p&gt;The blackburn theme provides easy to configure social links via HMTL partials. This allows users to enter their socials links like this in their &lt;code&gt;config.toml&lt;/code&gt; file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[social]
  twitter = &amp;quot;*&amp;quot;
  github = &amp;quot;*&amp;quot;
  stackoverflow = &amp;quot;*&amp;quot;
  linkedin = &amp;quot;*&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While this setup is very convenient for sites that are included with the default theme, it makes it slightly more difficult to add custom themes. However, it is certainly not impossible! First, let’s take a look at the HTML partial for the social links. This can be found at &lt;code&gt;themes/blackburn/layours/partials/social.html&lt;/code&gt;. This HTML file is arranged in the order that the social links will appear if they are included in the config file. Here is an example of a few of the built in blocks within this file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!-- Twitter --&amp;gt;
{{ with .Site.Social.twitter }}
&amp;lt;li class=&amp;quot;pure-menu-item&amp;quot;&amp;gt;
  &amp;lt;a class=&amp;quot;pure-menu-link&amp;quot; href=&amp;quot;https://twitter.com/{{ . }}&amp;quot; target=&amp;quot;_blank&amp;quot;&amp;gt;&amp;lt;i class=&amp;quot;fa fa-twitter-square fa-fw&amp;quot;&amp;gt;&amp;lt;/i&amp;gt;Twitter&amp;lt;/a&amp;gt;
&amp;lt;/li&amp;gt;
{{ end }}

&amp;lt;!-- Github --&amp;gt;
{{ with .Site.Social.github }}
&amp;lt;li class=&amp;quot;pure-menu-item&amp;quot;&amp;gt;
  &amp;lt;a class=&amp;quot;pure-menu-link&amp;quot; href=&amp;quot;https://github.com/{{ . }}&amp;quot; target=&amp;quot;_blank&amp;quot;&amp;gt;&amp;lt;i class=&amp;quot;fa fa-github-square fa-fw&amp;quot;&amp;gt;&amp;lt;/i&amp;gt;GitHub&amp;lt;/a&amp;gt;
&amp;lt;/li&amp;gt;
{{ end }}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, it follows a pretty standard format. We can create our own custom social for RStudio Community like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!-- RStudio Community --&amp;gt;
{{ with .Site.Social.rstudiocommunity }}
&amp;lt;li class=&amp;quot;pure-menu-item&amp;quot;&amp;gt;
  &amp;lt;a class=&amp;quot;pure-menu-link&amp;quot; href=&amp;quot;https://community.rstudio.com/u/{{ . }}&amp;quot; target=&amp;quot;_blank&amp;quot;&amp;gt;&amp;lt;img src = &amp;quot;/images/community-icon.png&amp;quot; id = &amp;quot;community-icon&amp;quot;&amp;gt;RStudio Community&amp;lt;/a&amp;gt;
&amp;lt;/li&amp;gt;
{{ end }}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are a few important changes that are worth noting here. First, we created a name for the social link that will used in the config file by changing the content of the first line to &lt;code&gt;.Site.Social.rstudiocommunity&lt;/code&gt;. Then we changed the &lt;code&gt;href&lt;/code&gt; link to the base url for an RStudio Community user with the partial syntax that will input the value from the config file - &lt;code&gt;&amp;quot;https://community.rstudio.com/u/{{ . }}&amp;quot;&lt;/code&gt;. We also had to change the text for the link to “RStudio Community”. Finally, we had to insert the image that we created in the first section of this post! That is done with this piece of code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;img src = &amp;quot;/images/community-icon.png&amp;quot; id = &amp;quot;community-icon&amp;quot;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The leading &lt;code&gt;/&lt;/code&gt; is important because it tells the site that it should always be looking for &lt;code&gt;images/&lt;/code&gt; folder in the base directory and not in an individual page directory. This is the same way you would reference an image from inside of a blog post! We also can add an id for our image so that we can add some custom styling to it (which we will cover next).&lt;/p&gt;
&lt;p&gt;Now we can go back to our &lt;code&gt;config.toml&lt;/code&gt; file and add a new line for our RStudio Community account! Now the social section will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[social]
  twitter = &amp;quot;*&amp;quot;
  github = &amp;quot;*&amp;quot;
  stackoverflow = &amp;quot;*&amp;quot;
  linkedin = &amp;quot;*&amp;quot;
  rstudiocommunity = &amp;quot;*&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;customize-css&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Customize CSS&lt;/h1&gt;
&lt;p&gt;Now if you re render your site, you will see that your RStudio Community tag is included in your sidebar menu! Unfortunately, since the link has a long name, it doesn’t fit perfectly with the default width of the blackburn’s side bar menu. Luckily, all this takes is a little manipulation of the sites CSS, more specifically, the &lt;code&gt;themes/blackburn/static/css/side-menu.css&lt;/code&gt; file. I am not going to copy and paste the whole file here, but I will include all of the blocks (and their line numbers) that I modified. If you are customizing for a different theme or social link, I would recommend using the developer tools in your browser to interactively play with different CSS inputs.&lt;/p&gt;
&lt;p&gt;Here are the blocks that were changed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/* 
Increase menu size from 150px to 175px
lines 31-38
*/
#layout.active #menu {
    left: 175px;
    width: 175px;
}

#layout.active .menu-link {
    left: 175px;
}


/* 
Increase padding between content and side menu and 
increase the max width of content
Also increase the padding between the header and side 
bar menu
lines 42-56
*/
.content {
    margin: 0 auto;
    padding: 0 2.5em;
    max-width: 1200px;
    margin-bottom: 50px;
    line-height: 1.6em;
}

.header {
     margin: 0;
     color: #333;
     text-align: center;
     padding: 2.5em 2.5em 0;
     border-bottom: 1px solid #eee;
 }
 
 
/* 
Again, increasing menu width from 150px to 175px
lines 82-93
*/
#menu {
  margin-left: -175px; /* &amp;quot;#menu&amp;quot; width */
  width: 175px;
  position: fixed;
  top: 0;
  left: 0;
  bottom: 0;
  z-index: 1000; /* so the menu or its navicon stays above all content */
  background: #191818;
  overflow-y: auto;
  -webkit-overflow-scrolling: touch;
}

/* 
Edit the responsive styles so that when the screen is small or large 
menu looks right. In here we alter the menu widths from 150 to 175 and 
the left padding from 2 to 2.5 for .header and .content
*/
@media (min-width: 48em) {

    .header,
    .content {
        padding-left: 2.5em;
        padding-right: 2em;
    }

    #layout {
        padding-left: 175px; /* left col width &amp;quot;#menu&amp;quot; */
        left: 0;
    }
    #menu {
        left: 175px;
    }

    .menu-link {
        position: fixed;
        left: 175px;
        display: none;
    }

    #layout.active .menu-link {
        left: 175px;
    }
}

@media (max-width: 48em) {
    /* Only apply this when the window is small. Otherwise, the following
    case results in extra padding on the left:
        * Make the window small.
        * Tap the menu to trigger the active state.
        * Make the window large again.
    */
    #layout.active {
        position: relative;
        left: 175px;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can add some custom styling for our community icon! This can be placed anywhere in this &lt;code&gt;side-menu.css&lt;/code&gt; file and looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#community-icon {
  padding-left:3px;
  margin-right:0.4em;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our final product looks like this&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;/images/sidebar-menu-example.png&#34; /&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;This post demonstrates how to add an RStudio Community (or any site really) social link to the provided links in the Blackburn theme. While this is specific to the Blackburn theme, a lot of Hugo themes use HTML partials to include things from the &lt;code&gt;config.toml&lt;/code&gt; file and this example could likely be extended to other themes after exploring the different files provided in a given themes &lt;code&gt;layout&lt;/code&gt; folder.&lt;/p&gt;
&lt;p&gt;I hope you consider using &lt;a href=&#34;http://community.rstudio.com&#34;&gt;RStudio Community&lt;/a&gt; for all of your R, RStudio, Shiny, and tidyverse related questions! In addition, adding a link to your blog page will help increase awareness of the site!&lt;/p&gt;
&lt;/div&gt;
</description>
  </item>
  
<item>
  <title>About</title>
  <link>/about/</link>
  <pubDate>Tue, 21 Aug 2018 00:00:00 +0000</pubDate>
  
<guid>/about/</guid>
  <description>&lt;p&gt;&lt;img src=&#34;/images/headshot-tyler-bradley-small.png&#34;, align = &#34;right&#34;/&gt;&lt;/p&gt;

&lt;p&gt;My Name is Tyler Bradley and I am an Environmental Engineer for the Philadelphia Water Department. I work mostly with drinking water distribution system water quality data. Most of my day to day work is focused on data science. Some of my responsibilites include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Manage and maintain an Ubuntu server used to host &lt;a href=&#34;https://www.rstudio.com/products/connect/&#34;&gt;RStudio Connect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Create and maintain an internal R package used to connect to and query data from multiple internal databases. The functions used to query data allow for quick and structured queries to be run against these databases programmatically. This package also contains several functions to be used in accordance with the data that is queried from the databases.&lt;/li&gt;
&lt;li&gt;Develop and maintain a PostgreSQL database&lt;/li&gt;
&lt;li&gt;Develop and deploy shiny apps and dashboards that allow for quick access to water quality data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am a PhD student at Drexel University studying Environmental Engineering, more specifically using data science to solve distribution system water quality issues. Some of my specific reseach interests include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Nitrification in drinking water distribution systems&lt;/li&gt;
&lt;li&gt;Drinking water distribution system microbiome and its impact on things such as nitrification&lt;/li&gt;
&lt;li&gt;Using data science to evaluate drinking water quality&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am huge fan of R and the tidyverse and all that it includes. Currently, I am a sustainer/moderator on the &lt;a href=&#34;http://community.rstudio.com/&#34;&gt;RStudio Community&lt;/a&gt; discourse site.&lt;/p&gt;

&lt;p&gt;Last, but certainly not least, I am a huge Arsenal fan (aka gooner, hence the name of the blog)!&lt;/p&gt;

&lt;p&gt;Please feel free to follow me on  &lt;a href=&#34;https://twitter.com/tycbrad&#34;&gt;twitter&lt;/a&gt; and connect with me on &lt;a href=&#34;https://www.linkedin.com/in/tyler-bradley-68707292/&#34;&gt;LinkedIn&lt;/a&gt;. Please check out my blog posts on &lt;a href=&#34;https://www.r-bloggers.com/&#34;&gt;R-Bloggers&lt;/a&gt; along with tons of other great R related blogs.&lt;/p&gt;

&lt;p&gt;This website was built with the &lt;a href=&#34;https://github.com/rstudio/blogdown&#34;&gt;blogdown&lt;/a&gt; R package.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;iframe src=&#34;https://giphy.com/embed/z2pppDya3mrXa&#34; width=&#34;480&#34; height=&#34;270&#34; frameBorder=&#34;0&#34; class=&#34;giphy-embed&#34; allowFullScreen&gt;&lt;/iframe&gt;&lt;p&gt;&lt;a href=&#34;https://giphy.com/gifs/arsenal-football-club-dennis-bergkamp-z2pppDya3mrXa&#34;&gt;Enjoy this classic goal!&lt;/a&gt;&lt;/p&gt;
&lt;/center&gt;&lt;/p&gt;
</description>
  </item>
  
<item>
  <title>Create a dynamic number of UI elements in Shiny with purrr</title>
  <link>/2018/08/10/create-a-dynamic-number-of-ui-elements-in-shiny-with-purrr/</link>
  <pubDate>Fri, 10 Aug 2018 00:00:00 +0000</pubDate>
  
<guid>/2018/08/10/create-a-dynamic-number-of-ui-elements-in-shiny-with-purrr/</guid>
  <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://purrr.tidyverse.org/&#34;&gt;&lt;code&gt;purrr&lt;/code&gt;&lt;/a&gt; is an incredibly powerful package that has greatly enhanced my R programming abilities. &lt;code&gt;purrr&lt;/code&gt; has applications in pretty much any situation. One of the most useful situations, IMHO, is in the creation of a dynamic number of &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;&lt;code&gt;shiny&lt;/code&gt;&lt;/a&gt; UI elements. This can be extremely useful if you want to be able to create a dynamic number of ui elements (whether that be inputs or outputs) based on either user selection or the data being used. This post will walk through how to create a dynamic number of plots depending on how many parameters the user selects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;Before we get started, let’s load the libraries we will be using.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dataRetrieval)
library(tidyverse)
library(lubridate)
library(shiny)
library(shinyjs)
library(plotly)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;In this post, I will be using the R package &lt;a href=&#34;https://github.com/USGS-R/dataRetrieval&#34;&gt;&lt;code&gt;dataRetrieval&lt;/code&gt;&lt;/a&gt; provided by the USGS to access their public API. If you wish to know more about how to use this package I would recommend checking out &lt;a href=&#34;https://cran.r-project.org/web/packages/dataRetrieval/vignettes/dataRetrieval.html&#34;&gt;the package’s vignette&lt;/a&gt;. The data used in this example app is daily water quality averages for three parameters (Temperature, Conductivity, and Dissolved Oxygen) measured in the Delaware River at the Ben Franklin Bridge in Philadelphia, PA. The code to pull the site info is placed at the top of the script outside of the ui and server code and looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# usgs site number of ben franklin bridge site
site &amp;lt;- &amp;quot;01467200&amp;quot;

site_info &amp;lt;- whatNWISdata(siteNumbers = site, service = &amp;quot;dv&amp;quot;, statCd = &amp;quot;00003&amp;quot;)

param_info &amp;lt;- site_info$parm_cd %&amp;gt;% unique() %&amp;gt;% readNWISpCode()


site_meta &amp;lt;- site_info %&amp;gt;% 
  select(site_no, station_nm, parm_cd) %&amp;gt;% 
  left_join(param_info %&amp;gt;% 
              select(parameter_cd, srsname, parameter_units), 
            by = c(&amp;quot;parm_cd&amp;quot; = &amp;quot;parameter_cd&amp;quot;)) %&amp;gt;% 
  # these are the parameters with data at this site 
  filter(parm_cd %in% c(&amp;quot;00010&amp;quot;, &amp;quot;00095&amp;quot;, &amp;quot;00300&amp;quot;))

param_choices &amp;lt;- site_meta$parm_cd
names(param_choices) &amp;lt;- site_meta$srsname&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The actual data is queried from the API using the &lt;code&gt;readNWISdv&lt;/code&gt; function and reformatted to be easy to graph inside an &lt;code&gt;eventReactive&lt;/code&gt; function at the top of the server code. This looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wq_data &amp;lt;- eventReactive(input$submit, {
      req(input$parameter, input$date)
      
      raw_data &amp;lt;- readNWISdv(
        siteNumbers = site, 
        parameterCd = input$parameter,
        startDate = input$date[[1]],
        endDate = input$date[[2]]
      )
      
      output &amp;lt;- raw_data %&amp;gt;% 
        select(-contains(&amp;quot;_cd&amp;quot;)) %&amp;gt;% 
        gather(key = &amp;quot;parameter&amp;quot;, value = &amp;quot;result&amp;quot;, contains(&amp;quot;X_&amp;quot;)) %&amp;gt;% 
        mutate(parameter = str_replace_all(parameter, &amp;quot;X_|_00003&amp;quot;, &amp;quot;&amp;quot;)) %&amp;gt;% 
        left_join(site_meta, by = c(&amp;quot;parameter&amp;quot; = &amp;quot;parm_cd&amp;quot;, &amp;quot;site_no&amp;quot;)) 
      
      return(output)
    })&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting function&lt;/h2&gt;
&lt;p&gt;Since the focus of this post is how to generate multiple plots and the data format after basic manipulation is the same for all three parameters, I have defined my plot generation as a function named &lt;code&gt;wq_plotly()&lt;/code&gt; outside of the server code. This functions looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wq_plotly &amp;lt;- function(data){
  data %&amp;gt;%
    plot_ly(
      x = ~Date,
      y = ~result,
      type = &amp;quot;scatter&amp;quot;,
      mode = &amp;quot;lines+markers&amp;quot;,
      marker = list(
        size = 4,
        color = &amp;quot;blue&amp;quot;
      ),
      line = list(
        color = &amp;quot;blue&amp;quot;
      ),
      hoverinfo = &amp;quot;text&amp;quot;,
      text = ~paste(
        &amp;quot;Site:&amp;quot;, station_nm,
        &amp;quot;&amp;lt;br&amp;gt;Parameter:&amp;quot;, srsname,
        &amp;quot;&amp;lt;br&amp;gt;Date Time:&amp;quot;, format(Date),
        &amp;quot;&amp;lt;br&amp;gt;Result:&amp;quot;, result,
        &amp;quot;&amp;lt;br&amp;gt;Units:&amp;quot;, parameter_units
      )
    ) %&amp;gt;%
    layout(
      title = paste(
        unique(data$station_nm), &amp;quot;&amp;lt;br&amp;gt;&amp;quot;, 
        unique(data$srsname), 
        paste0(&amp;quot;(&amp;quot;, unique(data$parameter_units), &amp;quot;)&amp;quot;)
      ),
      titlefont = list(
        size = 10
      ),
      xaxis = list(
        title = &amp;quot;&amp;quot;
      ),
      yaxis = list(
        title = &amp;quot;&amp;quot;
      ),
      margin = list(
        t = 40
      )
    )
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ui&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;UI&lt;/h1&gt;
&lt;p&gt;So the first part of the app is the ui code. This part is actually somewhat straightforward because all of the magic is gonna happen in our server code. We have to define our inputs for which parameters to graph (named &lt;code&gt;input$parameter&lt;/code&gt;) and the desired date range (named &lt;code&gt;input$date&lt;/code&gt;) and create an &lt;code&gt;actionButton&lt;/code&gt; so that the user controls when new graphs are generated (my personal preference). I create these all within a single &lt;code&gt;fluidRow&lt;/code&gt; with each in its own &lt;code&gt;column&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;After that, I create a new &lt;code&gt;fluidRow&lt;/code&gt; and simply have a &lt;code&gt;uiOutput&lt;/code&gt; (with an id of &lt;code&gt;graphs_ui&lt;/code&gt;) in it. This &lt;code&gt;uiOutput&lt;/code&gt; will be created in our server code and contain all of the ui elements for our plots.&lt;/p&gt;
&lt;p&gt;Here is the full ui code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ui &amp;lt;- shinyUI(
  fluidPage(
    tags$head(
      tags$style(HTML(&amp;#39;.shiny-input-container{margin-top: 20px;}&amp;#39;))
    ),
    div(
      fluidRow(
        column(
          4, 
          selectInput(
            inputId = &amp;quot;parameter&amp;quot;,
            label = &amp;quot;Select Parameter(s):&amp;quot;,
            choices = param_choices,
            multiple = TRUE
          )
        ),
        column(
          4,
          dateRangeInput(
            inputId = &amp;quot;date&amp;quot;,
            label = &amp;quot;Select Date Range:&amp;quot;,
            start = Sys.Date() - days(31),
            end = Sys.Date()
          )
        ),
        column(
          4, 
          actionButton(
            inputId = &amp;quot;submit&amp;quot;,
            label = &amp;quot;Apply Changes!&amp;quot;,
            style = &amp;quot;margin:40px;&amp;quot;
          )
        )
      ),
      fluidRow(
        div(
          id = &amp;quot;plot-container&amp;quot;,
          uiOutput(
            outputId = &amp;quot;graphs_ui&amp;quot;
          )
        )
      )
    )
  )
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;server&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Server&lt;/h1&gt;
&lt;p&gt;Now for the fun! The first part of the server code, which was shown above, is the &lt;code&gt;wq_data&lt;/code&gt; reactive expression to query our data set. The next part involves generating a reactive object that contains a list of our graphs (a vector would work too). This is important because it is what will be used to generate the different inputs. Since I defined my graphing function above, I can call that with a combination of &lt;code&gt;dplyr::group_by&lt;/code&gt;, &lt;code&gt;tidyr::nest&lt;/code&gt;, &lt;code&gt;dplyr::mutate&lt;/code&gt;, &lt;code&gt;purrr::map&lt;/code&gt;, and &lt;code&gt;dplyr::pull&lt;/code&gt;. This combination allows us to create unique graphs for each parameter and store them in a single list. The code looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;graphs &amp;lt;- eventReactive(input$submit, {
  req(wq_data())
      
  wq_data() %&amp;gt;% 
    group_by(parameter) %&amp;gt;% 
    nest() %&amp;gt;% 
    mutate(
       graphs = map(data, wq_plotly) 
    ) %&amp;gt;% 
    arrange(parameter) %&amp;gt;% 
    pull(graphs)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is important to note that I have only tested this on &lt;a href=&#34;https://plot.ly/r/&#34;&gt;&lt;code&gt;plotly&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://ggplot2.tidyverse.org/&#34;&gt;&lt;code&gt;ggplot2&lt;/code&gt;&lt;/a&gt;, and base graphics. This method only works with &lt;code&gt;plotly&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt;. It does not work with base graphics because base plots cannot be saved as objects.&lt;/p&gt;
&lt;p&gt;Now that we have our list of graphs, we need to create our outputs. However, since we are giving the user the ability to choose how many parameters they want to create graphs for, &lt;em&gt;how do we know&lt;/em&gt; how many outputs to create? With &lt;code&gt;purrr&lt;/code&gt; and our list of graphs, &lt;strong&gt;we don’t need to know&lt;/strong&gt; how many outputs need to be created. By iterating over our list of graphs with &lt;code&gt;iwalk&lt;/code&gt; we can create as many outputs as there are graphs.&lt;/p&gt;
&lt;p&gt;In this case, we want to use &lt;code&gt;purrr::iwalk&lt;/code&gt; (which is a variant of &lt;code&gt;walk&lt;/code&gt;) because we want to use both the graph and its index. Using &lt;code&gt;iwalk(x, ...)&lt;/code&gt; is the same as using &lt;code&gt;walk2(x, seq_along(x), ...)&lt;/code&gt;. We use &lt;code&gt;walk&lt;/code&gt; instead of &lt;code&gt;map&lt;/code&gt; because we are not returning anything from the overall iteration, but rather only using it for its side effects. Using &lt;code&gt;walk&lt;/code&gt; instead of &lt;code&gt;map&lt;/code&gt; is similar to using an &lt;code&gt;observe&lt;/code&gt; function instead of a &lt;code&gt;reactive&lt;/code&gt; function. To further connect the comparison, we will use our &lt;code&gt;iwalk&lt;/code&gt; function inside of an &lt;code&gt;observeEvent&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;Here is what the output generation looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;observeEvent(input$submit, {
  req(graphs())
      
  iwalk(graphs(), ~{
    output_name &amp;lt;- paste0(&amp;quot;plot_&amp;quot;, .y)
    output[[output_name]] &amp;lt;- renderPlotly(.x)
  })
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are two simple pieces to our &lt;code&gt;iwalk&lt;/code&gt; function. First, we define a unique &lt;code&gt;outputId&lt;/code&gt; using the index of the plot in our list. Then using that &lt;code&gt;outputId&lt;/code&gt; we can render our plotly object.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; defining an output using &lt;code&gt;output[[&amp;quot;my_output_id&amp;quot;]]&lt;/code&gt; is the same as defining it as &lt;code&gt;output$my_output_id&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, the last step of the server code is to create the ui elements! Remember, in our ui code, we defined the &lt;code&gt;uiOutput&lt;/code&gt; for our graphs with an id of &lt;code&gt;graphs_ui&lt;/code&gt;. So now, we will use &lt;code&gt;renderUI&lt;/code&gt; and &lt;code&gt;purrr::imap&lt;/code&gt; to create our ui elements. Since &lt;code&gt;render*&lt;/code&gt; functions are similar to &lt;code&gt;reactive&lt;/code&gt; functions, in that they return their output, we use &lt;code&gt;imap&lt;/code&gt; rather than &lt;code&gt;iwalk&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is what our ui generation looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;output$graphs_ui &amp;lt;- renderUI({
  req(graphs())
      
  plots_list &amp;lt;- imap(graphs(), ~{
    tagList(
      plotlyOutput(
        outputId = paste0(&amp;quot;plot_&amp;quot;, .y)
      ),
      br()
    )
        
  })
      
  tagList(plots_list)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, we use the index of our plot list again to call each individual plot &lt;code&gt;outputId&lt;/code&gt;. It is important to notice the &lt;code&gt;tagList(plots_list)&lt;/code&gt; call at the end of the &lt;code&gt;renderUI&lt;/code&gt; function. Since the output of &lt;code&gt;imap&lt;/code&gt; is a list, &lt;code&gt;plots_list&lt;/code&gt; is a list of ui elements and is not valid to be entered directly into the UI code. &lt;code&gt;tagList&lt;/code&gt; takes care of this for us and combines multiple ui elements into one.&lt;/p&gt;
&lt;p&gt;Combining all of these elements, our final shiny app looks like this:&lt;/p&gt;
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/images/dynamic-ui-elements/dynamic-graphs.gif&#34; /&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Being able to create a dynamic number of ui elements, whether they are inputs or outputs, is an incredibly powerful tool when trying to scale your shiny apps! The method shown here was applied to creating a dynamic number of graphs based on the users input, but it is certainly not limited to that! You can see an example of how to apply this to creating &lt;code&gt;textInput&lt;/code&gt; and &lt;code&gt;numericInput&lt;/code&gt; dynamically based on column names of an uploaded dataset in this &lt;a href=&#34;https://community.rstudio.com/t/creating-multiple-numeric-input-according-to-the-variables-of-an-uploaded-dataset/12293&#34;&gt;RStudio Community thread&lt;/a&gt;. Learning and using &lt;code&gt;purrr&lt;/code&gt; can &lt;strong&gt;&lt;em&gt;dramatically&lt;/em&gt;&lt;/strong&gt; increase your capabilities within general R programming and building shiny applications!&lt;/p&gt;
&lt;p&gt;Finally, the full code for the shiny app looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dataRetrieval)
library(tidyverse)
library(lubridate)
library(shiny)
library(shinyjs)
library(plotly)


# usgs site number of ben franklin bridge site
site &amp;lt;- &amp;quot;01467200&amp;quot;

site_info &amp;lt;- whatNWISdata(siteNumbers = site, service = &amp;quot;dv&amp;quot;, statCd = &amp;quot;00003&amp;quot;)

param_info &amp;lt;- site_info$parm_cd %&amp;gt;% unique() %&amp;gt;% readNWISpCode()


site_meta &amp;lt;- site_info %&amp;gt;% 
  select(site_no, station_nm, parm_cd) %&amp;gt;% 
  left_join(param_info %&amp;gt;% 
              select(parameter_cd, srsname, parameter_units), 
            by = c(&amp;quot;parm_cd&amp;quot; = &amp;quot;parameter_cd&amp;quot;)) %&amp;gt;% 
  filter(parm_cd %in% c(&amp;quot;00010&amp;quot;, &amp;quot;00095&amp;quot;, &amp;quot;00300&amp;quot;))

param_choices &amp;lt;- site_meta$parm_cd
names(param_choices) &amp;lt;- site_meta$srsname

wq_plotly &amp;lt;- function(data){
  data %&amp;gt;%
    plot_ly(
      x = ~Date,
      y = ~result,
      type = &amp;quot;scatter&amp;quot;,
      mode = &amp;quot;lines+markers&amp;quot;,
      marker = list(
        size = 4,
        color = &amp;quot;blue&amp;quot;
      ),
      line = list(
        color = &amp;quot;blue&amp;quot;
      ),
      hoverinfo = &amp;quot;text&amp;quot;,
      text = ~paste(
        &amp;quot;Site:&amp;quot;, station_nm,
        &amp;quot;&amp;lt;br&amp;gt;Parameter:&amp;quot;, srsname,
        &amp;quot;&amp;lt;br&amp;gt;Date Time:&amp;quot;, format(Date),
        &amp;quot;&amp;lt;br&amp;gt;Result:&amp;quot;, result,
        &amp;quot;&amp;lt;br&amp;gt;Units:&amp;quot;, parameter_units
      )
    ) %&amp;gt;%
    layout(
      title = paste(
        unique(data$station_nm), &amp;quot;&amp;lt;br&amp;gt;&amp;quot;, 
        unique(data$srsname), 
        paste0(&amp;quot;(&amp;quot;, unique(data$parameter_units), &amp;quot;)&amp;quot;)
      ),
      titlefont = list(
        size = 10
      ),
      xaxis = list(
        title = &amp;quot;&amp;quot;
      ),
      yaxis = list(
        title = &amp;quot;&amp;quot;
      ),
      margin = list(
        t = 40
      )
    )
}


ui &amp;lt;- shinyUI(
  fluidPage(
    tags$head(
      tags$style(HTML(&amp;#39;.shiny-input-container{margin-top: 20px;}&amp;#39;))
    ),
    div(
      fluidRow(
        column(
          4, 
          selectInput(
            inputId = &amp;quot;parameter&amp;quot;,
            label = &amp;quot;Select Parameter(s):&amp;quot;,
            choices = param_choices,
            multiple = TRUE
          )
        ),
        column(
          4,
          dateRangeInput(
            inputId = &amp;quot;date&amp;quot;,
            label = &amp;quot;Select Date Range:&amp;quot;,
            start = Sys.Date() - days(31),
            end = Sys.Date()
          )
        ),
        column(
          4, 
          actionButton(
            inputId = &amp;quot;submit&amp;quot;,
            label = &amp;quot;Apply Changes!&amp;quot;,
            style = &amp;quot;margin:40px;&amp;quot;
          )
        )
      ),
      fluidRow(
        div(
          id = &amp;quot;plot-container&amp;quot;,
          uiOutput(
            outputId = &amp;quot;graphs_ui&amp;quot;
          )
        )
      )
    )
  )
)


server &amp;lt;- shinyServer(
  function(input, output, session){
    session$onSessionEnded(stopApp)
    
    # query data from USGS API
    wq_data &amp;lt;- eventReactive(input$submit, {
      req(input$parameter, input$date)
      
      raw_data &amp;lt;- readNWISdv(
        siteNumbers = site, 
        parameterCd = input$parameter,
        startDate = input$date[[1]],
        endDate = input$date[[2]]
      )
      
      output &amp;lt;- raw_data %&amp;gt;% 
        select(-contains(&amp;quot;_cd&amp;quot;)) %&amp;gt;% 
        gather(key = &amp;quot;parameter&amp;quot;, value = &amp;quot;result&amp;quot;, contains(&amp;quot;X_&amp;quot;)) %&amp;gt;% 
        mutate(parameter = str_replace_all(parameter, &amp;quot;X_|_00003&amp;quot;, &amp;quot;&amp;quot;)) %&amp;gt;% 
        left_join(site_meta, by = c(&amp;quot;parameter&amp;quot; = &amp;quot;parm_cd&amp;quot;, &amp;quot;site_no&amp;quot;)) 
      
      return(output)
    })
    
    # create a list of graphs - with one for each parameter selected
    graphs &amp;lt;- eventReactive(input$submit, {
      req(wq_data())
      
      wq_data() %&amp;gt;% 
        group_by(parameter) %&amp;gt;% 
        nest() %&amp;gt;% 
        mutate(
          graphs = map(data, wq_plotly) 
        ) %&amp;gt;% 
        arrange(parameter) %&amp;gt;% 
        pull(graphs)
    })
    
    # use purrr::iwalk to create a dynamic number of outputs
    observeEvent(input$submit, {
      req(graphs())
      
      iwalk(graphs(), ~{
        output_name &amp;lt;- paste0(&amp;quot;plot_&amp;quot;, .y)
        output[[output_name]] &amp;lt;- renderPlotly(.x)
      })
    })
    
    # use renderUI to create a dynamic number of output ui elements
    output$graphs_ui &amp;lt;- renderUI({
      req(graphs())
      
      plots_list &amp;lt;- imap(graphs(), ~{
        tagList(
          plotlyOutput(
            outputId = paste0(&amp;quot;plot_&amp;quot;, .y)
          ),
          br()
        )
        
      })
      
      tagList(plots_list)
    })
    
  }
)


shinyApp(ui = ui, server = server)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
  </item>
  
<item>
  <title>Using global input values inside of R Shiny modules</title>
  <link>/2018/07/20/r-shiny-modules--using-global-inputs/</link>
  <pubDate>Fri, 20 Jul 2018 00:00:00 +0000</pubDate>
  
<guid>/2018/07/20/r-shiny-modules--using-global-inputs/</guid>
  <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This week, I have been working on a new shiny app. This app allows for similar figures to be rendered based on which summary statistic the user is interested in. There are four different figure types for the user to choose from, each of which are placed into their own &lt;code&gt;tabPanel&lt;/code&gt;. This means copying the server code used to generate the graphs four different times. Following Hadley’s words of wisdom in &lt;a href=&#34;http://r4ds.had.co.nz/functions.html&#34;&gt;R for Data Science&lt;/a&gt;, I wanted to abstract this process into functions. In the world of &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;shiny&lt;/a&gt;, this means working with &lt;a href=&#34;https://shiny.rstudio.com/articles/modules.html&#34;&gt;modules&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I have worked with modules before, but each time I have, the modules have been more overarching (i.e. they were large components of the app that did not share inputs) and I did not use them to repeat a similar process but simply to split up the code of a large app. In this case, I want the user to be able to select their inputs and click a submit button which will result in all of the graphs being updated. Then they will be able to switch between the &lt;code&gt;tabPanel&lt;/code&gt;s without having to click any additional buttons or re-select any inputs. This desired functionality requires using the same &lt;code&gt;actionButton&lt;/code&gt; and the same data set for multiple modules. However, I found out it isn’t as easy as I expected for global inputs to be used within modules!&lt;/p&gt;
&lt;p&gt;To illustrate my issue, I have put together a super simple app that has an action button and returns the count of the number of times it has been pressed. This count is output twice, one of the counts represents the count output being rendered outside of a module and then other shows the count output being rendered inside of a module. In addition, the module code returns a text output stating whether the count is odd or even. This demonstrates how this issue impacts &lt;code&gt;eventReactive&lt;/code&gt; functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;attempt-no.-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Attempt No. 1&lt;/h1&gt;
&lt;p&gt;On my first attempt, I simply abstracted my code so that the module would accept a &lt;code&gt;reactive&lt;/code&gt; data set and simply passed my &lt;code&gt;actionButton&lt;/code&gt; input (&lt;code&gt;input$submit&lt;/code&gt; - creative, I know) straight into the module code. In my simplified app, I did something similar (just without the data set) and it looked like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(shiny)

# attempt #1
count_module_ui &amp;lt;- function(id){
  ns &amp;lt;- NS(id)
  
  tagList(
    h4(&amp;quot;Modulized Count&amp;quot;),
    textOutput(ns(&amp;quot;count_inside&amp;quot;)),
    h4(&amp;quot;Is Modulized Count Odd or Even?&amp;quot;),
    textOutput(ns(&amp;quot;odd_even&amp;quot;))
  )
}

count_module &amp;lt;- function(input, output, session){
  
  output$count_inside &amp;lt;- renderText(as.character(input$submit))
  
  temp_text &amp;lt;- eventReactive(input$submit, {
    if (action() %% 2 == 0) {
      return(&amp;quot;even&amp;quot;)
    } else{
      return(&amp;quot;odd&amp;quot;)
    }
  })
  
  output$odd_even &amp;lt;- renderText(temp_text())
}

ui &amp;lt;- fluidPage(
  actionButton(
    &amp;quot;submit&amp;quot;, 
    &amp;quot;Press me&amp;quot;
  ),
  h4(&amp;quot;Count Regular&amp;quot;),
  textOutput(&amp;quot;count&amp;quot;),
  count_module_ui(&amp;quot;count_module&amp;quot;)
)

server &amp;lt;- function(input, output, session) {
  session$onSessionEnded(stopApp)
  output$count &amp;lt;- renderText(as.character(input$submit))
  callModule(count_module, &amp;quot;count_module&amp;quot;)
}

shinyApp(ui = ui, server = server)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I thought for sure this would work. After all, in normal R functions, if a variable is not defined in function’s local environment, it looks for it in its parent environment. I assumed, incorrectly, that modules would behave in a similar way, and since there is no &lt;code&gt;input$submit&lt;/code&gt; defined in the module code, it would look to its parent environment for it. However, when I run this app, only the &lt;code&gt;h4()&lt;/code&gt; portions of the module ui are there, no matter how many times I press the button!&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;/images/modules-action-button/attempt-1.gif&#34; alt=&#34;My first attempt&#34; /&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;attempt-no.-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Attempt No. 2&lt;/h1&gt;
&lt;p&gt;It turns out that modules only recognize &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;output&lt;/code&gt; objects that are explicitly defined in the module or passed as an argument to the module. So I then decided to try making the &lt;code&gt;input$submit&lt;/code&gt; object an argument in the module code. It looked like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(shiny)

# attempt 2
count_module_ui &amp;lt;- function(id){
  ns &amp;lt;- NS(id)
  
  tagList(
    h4(&amp;quot;Modulized Count&amp;quot;),
    textOutput(ns(&amp;quot;count_inside&amp;quot;)),
    h4(&amp;quot;Is Modulized Count Odd or Even?&amp;quot;),
    textOutput(ns(&amp;quot;odd_even&amp;quot;))
  )
}

count_module &amp;lt;- function(input, output, session, action){
  
  output$count_inside &amp;lt;- renderText(as.character(action))
  
  temp_text &amp;lt;- eventReactive(action, {
    if (action() %% 2 == 0) {
      return(&amp;quot;even&amp;quot;)
    } else{
      return(&amp;quot;odd&amp;quot;)
    }
  })
  
  output$odd_even &amp;lt;- renderText(temp_text())
}

ui &amp;lt;- fluidPage(
  actionButton(
    &amp;quot;submit&amp;quot;, 
    &amp;quot;Press me&amp;quot;
  ),
  h4(&amp;quot;Count Regular&amp;quot;),
  textOutput(&amp;quot;count&amp;quot;),
  count_module_ui(&amp;quot;count_module&amp;quot;)
)

server &amp;lt;- function(input, output, session) {
  session$onSessionEnded(stopApp)
  output$count &amp;lt;- renderText(as.character(input$submit))
  callModule(count_module, &amp;quot;count_module&amp;quot;, action = input$submit)
}

shinyApp(ui = ui, server = server)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that now there is an extra argument in my &lt;code&gt;count_module&lt;/code&gt; server function that I pass &lt;code&gt;input$submit&lt;/code&gt; to in my &lt;code&gt;server&lt;/code&gt; code.&lt;/p&gt;
&lt;p&gt;When I loaded the app, I thought for sure I had gotten it to work because now the modulized count shows up as zero when the app loads. Unfortunately, that hope faded fairly quick when I pressed the action button and my non-modulized count increased and my modulized count stayed at zero…&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;/images/modules-action-button/attempt-2.gif&#34; alt=&#34;My second attempt&#34; /&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;attempt-no.-3-success&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Attempt No. 3 (Success!)&lt;/h1&gt;
&lt;p&gt;Finally, I realized, thanks to &lt;a href=&#34;https://stackoverflow.com/questions/45169876/observeevent-shiny-function-used-in-a-module-does-not-work&#34;&gt;this StackOverflow answer&lt;/a&gt;, that I had to pass the &lt;code&gt;input$submit&lt;/code&gt; to the module as a &lt;em&gt;reactive&lt;/em&gt;. This wasn’t clear to me at first, since inputs are typically treated as dynamic elements. So to correct for this, I created a separate reactive element in my server code, &lt;code&gt;count_value&lt;/code&gt;. This is simply the value of &lt;code&gt;input$submit&lt;/code&gt;. I then passed this reactive element to my module. Here is the updated code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(shiny)

# attempt 3
count_module_ui &amp;lt;- function(id){
  ns &amp;lt;- NS(id)
  
  tagList(
    h4(&amp;quot;Modulized Count&amp;quot;),
    textOutput(ns(&amp;quot;count_inside&amp;quot;)),
    h4(&amp;quot;Is Modulized Count Odd or Even?&amp;quot;),
    textOutput(ns(&amp;quot;odd_even&amp;quot;))
  )
}

count_module &amp;lt;- function(input, output, session, action){
  
  output$count_inside &amp;lt;- renderText(as.character(action()))
  
  temp_text &amp;lt;- eventReactive(action(), {
    if (action() %% 2 == 0) {
      return(&amp;quot;even&amp;quot;)
    } else{
      return(&amp;quot;odd&amp;quot;)
    }
  })
  
  output$odd_even &amp;lt;- renderText(temp_text())
}

ui &amp;lt;- fluidPage(
  actionButton(
    &amp;quot;submit&amp;quot;, 
    &amp;quot;Press me&amp;quot;
  ),
  h4(&amp;quot;Count Regular&amp;quot;),
  textOutput(&amp;quot;count&amp;quot;),
  count_module_ui(&amp;quot;count_module&amp;quot;)
)


server &amp;lt;- function(input, output, session) {
  session$onSessionEnded(stopApp)
  output$count &amp;lt;- renderText(as.character(input$submit))
  count_value &amp;lt;- reactive(input$submit)
  callModule(count_module, &amp;quot;count_module&amp;quot;, action = count_value)
}

shinyApp(ui = ui, server = server)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;At last, Success!!&lt;/strong&gt;&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;/images/modules-action-button/attempt-3.gif&#34; alt=&#34;At last, success!!&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;Notice that in addition to adding the &lt;code&gt;count_value&lt;/code&gt; reactive expression to my server code, I also passed this value to the module with out the normal &lt;code&gt;()&lt;/code&gt; at the end of a reactive call. That is because the &lt;code&gt;()&lt;/code&gt; at the end of this value is then passed to &lt;code&gt;action&lt;/code&gt; &lt;em&gt;in&lt;/em&gt; the module server code. So instead of &lt;code&gt;action = count_value()&lt;/code&gt; in the &lt;code&gt;callModule&lt;/code&gt; function, you pass &lt;code&gt;action = count_value&lt;/code&gt; to &lt;code&gt;callModule&lt;/code&gt; and then inside of the &lt;code&gt;count_module&lt;/code&gt; function you call &lt;code&gt;action()&lt;/code&gt; instead of &lt;code&gt;action&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Using modules can be an extremely useful tool for both breaking up large apps but also when trying to avoid repetition in your server code. However, abstracting your shiny code to modules it not always as straight forward as abstracting your code to functions (not that that is always straightforward, either!). This post demonstrates a useful trick when you don’t want to create repetitive &lt;code&gt;actionButton&lt;/code&gt; input types (or any input for that matter) for each module but rather want modules to share input values!&lt;/p&gt;
&lt;/div&gt;
</description>
  </item>
  
<item>
  <title>Subsetting Phylogenetic Trees</title>
  <link>/2018/06/19/subsetting-phylogenetic-trees/</link>
  <pubDate>Tue, 19 Jun 2018 00:00:00 +0000</pubDate>
  
<guid>/2018/06/19/subsetting-phylogenetic-trees/</guid>
  <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In the past few months, I have been introduced to and started working with phylogenetic trees. The trees I have been working with contain the entire tree of life for bacteria. Needless to say, they are huge. At first, I was told that I could download a program that would let me view the tree and search for particular species on the tree, but of course, I immediately looked for a better solution using R. As is the case for most things, I was able to find a full suite of packages (&lt;a href=&#34;https://github.com/GuangchuangYu/treeio&#34;&gt;&lt;code&gt;treeio&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/GuangchuangYu/ggtree&#34;&gt;&lt;code&gt;ggtree&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://github.com/GuangchuangYu/tidytree&#34;&gt;&lt;code&gt;tidytree&lt;/code&gt;&lt;/a&gt; - all created by &lt;a href=&#34;https://guangchuangyu.github.io&#34;&gt;Guangchuang Yu&lt;/a&gt;) that allowed me to interact with this data.&lt;/p&gt;
&lt;p&gt;However, since my tree was so large, it was extremely difficult to make legible figures even with the great flexibility of &lt;code&gt;ggtree&lt;/code&gt;. In order to better analyze the tree as I needed to, I needed to come up with a way to look at only the portions of the tree that I wanted while still maintaining the structure of the tree for the subset portion. As a result, I created the &lt;code&gt;tree_subset&lt;/code&gt; function which is now in the development version (1.5.1.002) of &lt;code&gt;treeio&lt;/code&gt; on github. This post will briefly walk through the usage of this function!&lt;/p&gt;
&lt;p&gt;Before we get started, let’s load in the required packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install development version of treeio
# devtools::install_github(&amp;quot;GuangchuangYu/treeio&amp;quot;)

library(ape)
library(treeio)
library(ggplot2)
library(ggtree)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;subset-trees-by-tip-label&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Subset trees by tip label&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;tree_subset()&lt;/code&gt; allows the user to subset a tree, either of class &lt;code&gt;phylo&lt;/code&gt; or &lt;code&gt;treedata&lt;/code&gt;, by specifying the tip label they are interested in and the number of levels back they want to go. To demonstrate, let’s generate a random tree.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)

bi_tree &amp;lt;- rtree(100)
bi_tree$tip_label &amp;lt;- paste0(&amp;quot;t&amp;quot;, 1:100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To show why this function is useful, let’s first try to visualize this tree that has 100 tips.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bi_tree %&amp;gt;% 
  ggtree() + 
  geom_tiplab() +
  theme_tree2()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-19-subsetting-phylogenetic-trees_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the tree can get slightly crowded. Obviously, we could make the figure taller to allow more space for the labels and we could make the text smaller. However, these fixes are not always applicable when you have a lot more than 100 tips. Sometimes you have hundreds or thousands of tips but you are only interested in the portion of the tree around a particular tip. That is where &lt;code&gt;tree_subset&lt;/code&gt; comes in. Let’s say we are interested in tip &lt;code&gt;t79&lt;/code&gt; from the above tree and we want to look at the immediate relatives of this tip.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bi_subset &amp;lt;- tree_subset(bi_tree, &amp;quot;t79&amp;quot;, levels_back = 4)

bi_subset %&amp;gt;% 
  ggtree(aes(color = group)) + 
  geom_tiplab() + 
  theme_tree2() + 
  scale_color_manual(values = c(`1` = &amp;quot;red&amp;quot;, `0` = &amp;quot;black&amp;quot;)) +
  xlim(0, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-19-subsetting-phylogenetic-trees_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This function allows for you to easily look at portions of the tree that are of interest. This can greatly reduce the amount of time spent if trying to investigate a large tree for certain species. A couple of important things to note about the function above. As you may have noticed, the specified tip is grouped separately then the rest to allow for it to be easily distinguishable from the rest of the tips. If your tree has existing groups, this functionality can be disabled by specifying &lt;code&gt;group_node = FALSE&lt;/code&gt; in the &lt;code&gt;tree_subset&lt;/code&gt; call. Additionally, the branch lengths of the different nodes is maintained after the subset, however, the root of the tree is always anchored at zero for the subset tree, so all distances are relative to this root.&lt;/p&gt;
&lt;p&gt;In addition to working on &lt;code&gt;phylo&lt;/code&gt; objects, this function works also on &lt;code&gt;treedata&lt;/code&gt; objects and maintains all of the information associated with the &lt;code&gt;treedata&lt;/code&gt; object. Here is an example with a &lt;code&gt;treedata&lt;/code&gt; object from the &lt;code&gt;ggtree&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beast_file &amp;lt;- system.file(&amp;quot;examples/MCC_FluA_H3.tree&amp;quot;, package=&amp;quot;ggtree&amp;quot;)
rst_file &amp;lt;- system.file(&amp;quot;examples/rst&amp;quot;, package=&amp;quot;ggtree&amp;quot;)
mlc_file &amp;lt;- system.file(&amp;quot;examples/mlc&amp;quot;, package=&amp;quot;ggtree&amp;quot;)
beast_tree &amp;lt;- read.beast(beast_file)
codeml_tree &amp;lt;- read.codeml(rst_file, mlc_file)

merged_tree &amp;lt;- merge_tree(beast_tree, codeml_tree)

merged_tree %&amp;gt;% 
  ggtree() + 
  geom_tiplab(size = 3) +
  theme_tree2() +
  lims(x = c(0, 30))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-19-subsetting-phylogenetic-trees_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The tree is pretty crowded when trying to view the full tree. Let’s see what it looks like using &lt;code&gt;tree_subset&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;merged_subset &amp;lt;- tree_subset(merged_tree, &amp;quot;A/Swine/GX/2242/2011&amp;quot;, levels_back = 4)

merged_subset %&amp;gt;%
  ggtree(aes(color = group)) + 
  geom_tiplab(size = 3) +
  scale_color_manual(values = c(`1` = &amp;quot;red&amp;quot;, `0` = &amp;quot;black&amp;quot;)) +
  theme_tree2() +
  lims(x = c(0, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-19-subsetting-phylogenetic-trees_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shiny-app-to-explore-phylogentic-trees&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Shiny App to explore phylogentic trees&lt;/h1&gt;
&lt;p&gt;In addition to the &lt;code&gt;tree_subset&lt;/code&gt; function in the &lt;code&gt;treeio&lt;/code&gt; package. I have created a fairly basic shiny application that allows for users to interactively import a tree object and select different nodes that they want to subset the tree by. The code and instructions for running it can be found &lt;a href=&#34;https://github.com/tbradley1013/tree-subset-shiny&#34;&gt;here&lt;/a&gt;. This app allows you to import most of the tree files types that &lt;code&gt;treeio&lt;/code&gt; has methods for (see &lt;a href=&#34;http://bioconductor.org/packages/devel/bioc/vignettes/treeio/inst/doc/Importer.html&#34;&gt;here&lt;/a&gt; for info on different tree file types). Here is an example of the app:&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;/images/tree-subset-gif.gif&#34; /&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;This function allows users to explore their phylogenetic trees by looking at specific portions of the overall tree when the full tree is too large to easily interpret. The shiny application allows for users who are not familiar with R to easily use this function so that they can explore the trees in a method that, in my opinion, is more efficient than some of the other phylogentic tree programs. I hope that this function is of use to others!&lt;/p&gt;
&lt;p&gt;This was my first experience with contributing to an open-source project and Dr. Yu made it extremely easy. I would like to thank him for helping me get my pull request merged and for his great packages!&lt;/p&gt;
&lt;/div&gt;
</description>
  </item>
  
<item>
  <title>PCA in a tidy(verse) framework</title>
  <link>/2018/02/01/pca-in-a-tidy-verse-framework/</link>
  <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
  
<guid>/2018/02/01/pca-in-a-tidy-verse-framework/</guid>
  <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The other day, a &lt;a href=&#34;https://community.rstudio.com/t/tidyverse-solutions-for-factor-analysis-principal-component-analysis/4504&#34;&gt;question&lt;/a&gt; was posted on &lt;a href=&#34;https://community.rstudio.com/&#34;&gt;RStudio Community&lt;/a&gt; about performing Principal Component Analysis (PCA) in a &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; workflow. Conveniently, I had literally just worked through this process the day before and was able to post an &lt;a href=&#34;https://community.rstudio.com/t/tidyverse-solutions-for-factor-analysis-principal-component-analysis/4504/2&#34;&gt;answer&lt;/a&gt;. While most questions and answers are good as they are on forum sites, I thought this one might be worth exploring a little more since using the tidyverse framework makes PCA much easier, in my opinion.&lt;/p&gt;
&lt;p&gt;PCA is a multi-variate statistical technique for dimension reduction. Essentially, it allows you to take a data set that has n continuous variables and relate them through n orthogonal dimensions. This is a method of unsupervised learning that allows you to better understand the variability in the data set and how different variables are related. While there are the same number of principal components created as there are variables (assuming you have more observations than variables-but that is another issue), each principal component explains the maximum possible variation in the data conditional on it being orthogonal, or perpendicular, to the previous principal components.&lt;/p&gt;
&lt;p&gt;In my answer, I used the &lt;code&gt;iris&lt;/code&gt; data set to demonstrate how PCA can be done in the tidyverse workflow. For this post, I will be using the &lt;code&gt;USArrests&lt;/code&gt; data set that was used in &lt;a href=&#34;http://www-bcf.usc.edu/~gareth/ISL/index.html&#34;&gt;An Introduction to Statistical Thinking&lt;/a&gt; by Gareth James et. al. In this book, they work through a PCA and focus on the statistics and explanations behind PCA. This is how I learned how to do PCA and would highly recommend it if you are unfamiliar with the topic. In this blog post, my focus will be more on implementing the PCA in the tidyverse framework. Another nice walkthough of PCA with this dataset that is online can be found at &lt;a href=&#34;http://uc-r.github.io/pca&#34;&gt;University of Cincinnati’s R blog&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exploring the data&lt;/h1&gt;
&lt;p&gt;Before we dive in to the analysis, we want to explore our data set and become familiar with it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(broom)
library(knitr)
library(ggfortify)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;USArrests %&amp;gt;% head() %&amp;gt;% knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Murder&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Assault&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;UrbanPop&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Rape&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Alabama&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;236&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;58&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Alaska&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;263&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;48&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;44.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Arizona&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;294&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;31.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Arkansas&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;190&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;California&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;276&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;91&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;40.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Colorado&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;204&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;78&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;38.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Looking at the first 6 rows using the &lt;code&gt;head()&lt;/code&gt; function, we can see that each row is a state and and each column is a variable. Looking at &lt;code&gt;?USArrests&lt;/code&gt;, we can see that the column descriptions are as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Murder - the number of murder arrests per 100,000 people in a given state&lt;/li&gt;
&lt;li&gt;Assault - the number of assault arrests per 100,000 people in a given state&lt;/li&gt;
&lt;li&gt;UrbanPop - a numeric percentage of the urban population per state (i.e. the percentage of the state’s population that lives in cities)&lt;/li&gt;
&lt;li&gt;Rape - the number of rape arrests per 100,000 people in a given state&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, that we see how the data set is set up, let’s try to visualize the data as it is. Before we do that, let’s convert the data set to a &lt;code&gt;tibble&lt;/code&gt;. Since tibbles do not support rownames, we will have to convert them to their own column with &lt;code&gt;rownames_to_column&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;us_arrests &amp;lt;- USArrests %&amp;gt;% 
  rownames_to_column(var = &amp;quot;state&amp;quot;) %&amp;gt;% 
  # I prefer column names to be all lowercase so I am going to change them here
  rename_all(tolower) %&amp;gt;% 
  as_tibble()

us_arrests&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 50 x 5
##    state       murder assault urbanpop  rape
##    &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;   &amp;lt;int&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Alabama      13.2      236       58  21.2
##  2 Alaska       10.0      263       48  44.5
##  3 Arizona       8.10     294       80  31.0
##  4 Arkansas      8.80     190       50  19.5
##  5 California    9.00     276       91  40.6
##  6 Colorado      7.90     204       78  38.7
##  7 Connecticut   3.30     110       77  11.1
##  8 Delaware      5.90     238       72  15.8
##  9 Florida      15.4      335       80  31.9
## 10 Georgia      17.4      211       60  25.8
## # ... with 40 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at murder arrest rates in each of the stats.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;us_arrests %&amp;gt;% 
  mutate(state = factor(state), 
         state = fct_reorder(state, murder) %&amp;gt;% fct_rev()) %&amp;gt;% 
  ggplot(aes(state, murder)) + 
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.4)) +
  labs(y = &amp;quot;Murder Arrest Rate per 100,000 people&amp;quot;,
       x = &amp;quot;State&amp;quot;,
       title = &amp;quot;Murder rate in each state of the USA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-01-pca-in-a-tidy-verse-framework_files/figure-html/murder-bar-plot-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can see that Georgia has the highest murder rate, followed by Mississippi, Louisiana, and Florida Let’s try one more.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;us_arrests %&amp;gt;% 
  gather(key = crime, value = rate, c(murder, assault, rape)) %&amp;gt;% 
  ggplot(aes(urbanpop, rate, color = crime)) + 
  facet_wrap(~crime, scales = &amp;quot;free_y&amp;quot;, ncol = 1) +
  geom_point() + 
  geom_smooth(se = FALSE, method = &amp;quot;lm&amp;quot;) +
  theme_bw() + 
  labs(x = &amp;quot;Percentage Urban Population&amp;quot;,
       y = &amp;quot;Arrest Rate per 100,000 people&amp;quot;,
       title = &amp;quot;Arrest rate vs percentage urban population&amp;quot;) +
  theme(legend.title = element_blank(),
        legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-01-pca-in-a-tidy-verse-framework_files/figure-html/tidy-us-arrests-plot-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This figure is a slightly more informative than the last one. In this figure, each crime rate is plotted against percentage urban population. Simple linear models are fit for each of the different crime types to see if any pattern can be seen in the data. There appears to be an increase in assault arrests as urban population grows, however, there is &lt;em&gt;a lot&lt;/em&gt; of variability around the line of best fit. Rape arrest rates follow the linear model much closer than the others but there is still a lot of variability. On the other hand, murder arrest rates seem to be unchanged based on urban population.&lt;/p&gt;
&lt;p&gt;Now we could certainly do correlations, multiple linear regressions, or fit other types of models and would likely gain some useful insights, but instead let’s focus on the PCA.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pca-in-the-tidyverse-framework&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;PCA in the tidyverse framework&lt;/h1&gt;
&lt;p&gt;Now, when I first fit PCA models in R, I found myself with an unmanageable number of variables and to track and maintain. This can make the process overwhelming and can make you lose track of information. Luckily, using the &lt;code&gt;tidyverse&lt;/code&gt; and the &lt;code&gt;broom&lt;/code&gt; package, we can solve these issues much more easily. In order to run the model in the tidyverse framework, we will use the &lt;code&gt;nest()&lt;/code&gt; function along with the &lt;code&gt;mutate&lt;/code&gt; and &lt;code&gt;map&lt;/code&gt; combo to operate on the nested columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;us_arrests_pca &amp;lt;- us_arrests %&amp;gt;% 
  nest() %&amp;gt;% 
  mutate(pca = map(data, ~ prcomp(.x %&amp;gt;% select(-state), 
                                  center = TRUE, scale = TRUE)),
         pca_aug = map2(pca, data, ~augment(.x, data = .y)))

us_arrests_pca&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   data              pca          pca_aug               
##   &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;       &amp;lt;list&amp;gt;                
## 1 &amp;lt;tibble [50 x 5]&amp;gt; &amp;lt;S3: prcomp&amp;gt; &amp;lt;data.frame [50 x 10]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, we now have a tibble with one row and three columns. The first is our original data set, the second is the &lt;code&gt;prcomp&lt;/code&gt; object and the third is a data frame containing the principal component values for each observation. Now we have everything we need to evaluate the results of the model. First, it is important to look at how much variance is explained by each principal component. This will tell you how many of the components you need to look at when analyzing the results. To do this, you can use the data in the &lt;code&gt;pca_aug&lt;/code&gt; column of our &lt;code&gt;us_arrests_pca&lt;/code&gt; tibble along with some &lt;code&gt;dplyr&lt;/code&gt; functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var_exp &amp;lt;- us_arrests_pca %&amp;gt;% 
  unnest(pca_aug) %&amp;gt;% 
  summarize_at(.vars = vars(contains(&amp;quot;PC&amp;quot;)), .funs = funs(var)) %&amp;gt;% 
  gather(key = pc, value = variance) %&amp;gt;% 
  mutate(var_exp = variance/sum(variance),
         cum_var_exp = cumsum(var_exp),
         pc = str_replace(pc, &amp;quot;.fitted&amp;quot;, &amp;quot;&amp;quot;))

var_exp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 4
##   pc    variance var_exp cum_var_exp
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 PC1      2.48   0.620        0.620
## 2 PC2      0.990  0.247        0.868
## 3 PC3      0.357  0.0891       0.957
## 4 PC4      0.173  0.0434       1.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the first principal component explains 62% of the variation and that the second principal component explains 25% of the remaining variation. Together, we can see that they explain 87% of the variance in the data set. As a general rule of thumb, you want to look at enough principal components to explain ~90% of your data’s variability.&lt;/p&gt;
&lt;p&gt;Another way of assessing this is to plot the variance explained and the cumulative variance explained and look for the “elbow” in the graph. You can do that like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var_exp %&amp;gt;% 
  rename(
    `Variance Explained` = var_exp,
    `Cumulative Variance Explained` = cum_var_exp
  ) %&amp;gt;% 
  gather(key = key, value = value, `Variance Explained`:`Cumulative Variance Explained`) %&amp;gt;% 
  ggplot(aes(pc, value, group = key)) + 
  geom_point() + 
  geom_line() + 
  facet_wrap(~key, scales = &amp;quot;free_y&amp;quot;) +
  theme_bw() +
  lims(y = c(0, 1)) +
  labs(y = &amp;quot;Variance&amp;quot;,
       title = &amp;quot;Variance explained by each principal component&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-01-pca-in-a-tidy-verse-framework_files/figure-html/var-exp-graph-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since the majority of the variance is explained by the first two principal components, let’s plot them against each other. Luckily, this is made easy by the &lt;code&gt;ggplot2&lt;/code&gt; and &lt;code&gt;ggfortify&lt;/code&gt; packages which gives an &lt;code&gt;autoplot&lt;/code&gt; method for prcomp objects. Conveniently, we still have our &lt;code&gt;prcomp&lt;/code&gt; object stored in the our &lt;code&gt;us_arrests_pca&lt;/code&gt; tibble along with our original data. This is important as the original data is needed to add labels and/or colors to your ggplot based on discrete variables not included in the PCA. Using the same &lt;code&gt;mutate&lt;/code&gt; and &lt;code&gt;map&lt;/code&gt; combo as before, along with the handy &lt;code&gt;dplyr::pull&lt;/code&gt; function, we can plot this graph. &lt;code&gt;autoplot.prcomp()&lt;/code&gt; can take any arguments that can be passed to &lt;code&gt;ggbiplot()&lt;/code&gt;, so to see all of your options use &lt;code&gt;?ggbiplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;us_arrests_pca %&amp;gt;%
  mutate(
    pca_graph = map2(
      .x = pca,
      .y = data,
      ~ autoplot(.x, loadings = TRUE, loadings.label = TRUE,
                 loadings.label.repel = TRUE,
                 data = .y, label = TRUE,
                 label.label = &amp;quot;state&amp;quot;,
                 label.repel = TRUE) +
        theme_bw() +
        labs(x = &amp;quot;Principal Component 1&amp;quot;,
             y = &amp;quot;Principal Component 2&amp;quot;,
             title = &amp;quot;First two principal components of PCA on USArrests dataset&amp;quot;)
    )
  ) %&amp;gt;%
  pull(pca_graph)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-02-01-pca-in-a-tidy-verse-framework_files/figure-html/pca-plot-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you notice the &lt;code&gt;[[1]]&lt;/code&gt; above the graph, this method would automatically print each graph in the &lt;code&gt;pca_graph&lt;/code&gt; column. This means that if you fit multiple pca graphs onto a grouped data set then this would automatically print all of the figures for you in one command. Another important thing to notice is the the &lt;code&gt;autoplot()&lt;/code&gt; function simply returns a &lt;code&gt;gg&lt;/code&gt; object and that can be extended just like any other &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Looking at this figure, we are getting a lot more information about how these variables are related. For example, we can see that murder, rape, and assault arrest rates all have similar values alone the first principal component, which indicates that they are correlated with one another. We may have been able to figure that out using more basic methods, but this method allows us to see it in one figure. In addition, we are able to see how each of the observations in the data (i.e. states) relates to the variables. For example, California is characterized by high arrest rates (1st principal component) and high urban population (2nd principal component).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;PCA can provide an analysis with a lot of information about their data set and can give valuable insights into potentially unseen relationships between the observations and the variables. By performing this analysis in the tidyverse framework, we can easily extend this model by using the modelling capabilities of the tidyverse. You can see some of these &lt;a href=&#34;http://r4ds.had.co.nz/many-models.html&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://www.business-science.io/code-tools/2017/10/24/demo_week_timetk.html&#34;&gt;here&lt;/a&gt;. I believe that this framework allows for much more flexibility in your analysis and how you use the data from the model, which will enable you to gather more from your data in a faster and convenient way.&lt;/p&gt;
&lt;p&gt;Please let me know what you think of performing PCA in the tidyverse framework!&lt;/p&gt;
&lt;/div&gt;
</description>
  </item>
  
<item>
  <title>Creating, Writing, Querying, and Modifying SQL Database from R using odbc, dbplyr, and DBI</title>
  <link>/2017/08/26/sql-management-in-r/</link>
  <pubDate>Sat, 26 Aug 2017 00:00:00 +0000</pubDate>
  
<guid>/2017/08/26/sql-management-in-r/</guid>
  <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Recently, I have been building shiny apps for work. The app that I am currently working on is an interface to a database for storing information about laboratory samples being collected. In addition to building the shiny app for my coworkers to interact with the database, I also was tasked with creating and building the database. I have never build a SQL database from scratch, but luckily the &lt;a href=&#34;https://github.com/rstats-db/odbc&#34;&gt;odbc&lt;/a&gt; and the &lt;a href=&#34;https://github.com/rstats-db/DBI&#34;&gt;DBI&lt;/a&gt; packages make it fairly straight foreward.&lt;/p&gt;
&lt;p&gt;Let’s start by loading in the packages that we will need.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(DBI)
library(odbc)
library(RSQLite)
library(tidyverse)
library(magrittr)
library(dbplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;connecting-to-the-database&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Connecting to the Database&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;dbConnect&lt;/code&gt; function from the &lt;code&gt;DBI&lt;/code&gt; package allows us to create a SQLite database directly from R. SQLite databases are saved as files in the current working directory with this method. As described in the &lt;code&gt;RSQLite&lt;/code&gt; packge vignette, if you simply want to use a temporary database, you can create either an on-disk database or an in-memory database with this same method. For this example, we will create a new SQLite in-memory database&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;con &amp;lt;- dbConnect(RSQLite::SQLite(), &amp;quot;:memory:&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Currently, our database is empty, as can be seen if we use the &lt;code&gt;dbListTables&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dbListTables(con)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## character(0)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;writing-tables-to-database&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Writing Tables to database&lt;/h1&gt;
&lt;p&gt;To add data to this database we will use the &lt;code&gt;dbWriteTable&lt;/code&gt; function. First, let’s load in two common datasets, &lt;code&gt;mtcars&lt;/code&gt; and &lt;code&gt;diamonds&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;mtcars&amp;quot;)
data(&amp;quot;diamonds&amp;quot;)

mtcars %&amp;lt;&amp;gt;% 
  rownames_to_column()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have these two data sets loaded into the session, lets write them into the database.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dbWriteTable(con, &amp;quot;cars&amp;quot;, mtcars)
dbWriteTable(con, &amp;quot;diamonds&amp;quot;, diamonds)

dbListTables(con)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;cars&amp;quot;     &amp;quot;diamonds&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;query-the-database&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Query the Database&lt;/h1&gt;
&lt;p&gt;There are several ways that we can query the tables in this database. We can read in the entire table using the &lt;code&gt;dbReadTable&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dbReadTable(con, &amp;quot;cars&amp;quot;) %&amp;gt;%
  head(10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              rowname  mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## 1          Mazda RX4 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
## 2      Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
## 3         Datsun 710 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
## 4     Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
## 5  Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
## 6            Valiant 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
## 7         Duster 360 14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
## 8          Merc 240D 24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
## 9           Merc 230 22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
## 10          Merc 280 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Alternatively, we can write out a full sql query using the &lt;code&gt;dbGetQuery&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dbGetQuery(con, &amp;quot;select * from cars&amp;quot;) %&amp;gt;%
  head(10) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              rowname  mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## 1          Mazda RX4 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
## 2      Mazda RX4 Wag 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
## 3         Datsun 710 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
## 4     Hornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
## 5  Hornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
## 6            Valiant 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
## 7         Duster 360 14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
## 8          Merc 240D 24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
## 9           Merc 230 22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
## 10          Merc 280 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the &lt;code&gt;microbenchmark&lt;/code&gt; package to determine which of these methods is faster. We will measure the time for the diamonds data set as that has nearly 54,000 observations, as opposed to the 32 in the mtcars dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark::microbenchmark(
  read_table = dbReadTable(con, &amp;quot;diamonds&amp;quot;),
  query = dbGetQuery(con, &amp;quot;select * from diamonds&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##        expr      min       lq     mean   median       uq      max neval
##  read_table 91.36283 94.55963 115.4623 98.51866 150.1218 175.2671   100
##       query 90.61233 93.50576 111.0308 98.11973 113.0769 181.2567   100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like the &lt;code&gt;dbReadTable&lt;/code&gt; method is slightly faster than a full query. However, the real benefit to using &lt;code&gt;dbGetQuery&lt;/code&gt; is the ability to write much more complex sql queries. For example, if we want to subset the data, we are able to.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;query &amp;lt;- paste(&amp;quot;select carat, cut, clarity, color, price from diamonds&amp;quot;,
               &amp;quot;where carat &amp;gt; 1 and cut = &amp;#39;Ideal&amp;#39;&amp;quot;)
dbGetQuery(con, query) %&amp;gt;%
  as.tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5,662 x 5
##    carat   cut clarity color price
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt;
##  1  1.01 Ideal      I1     I  2844
##  2  1.02 Ideal     SI2     H  2856
##  3  1.02 Ideal      I1     I  2872
##  4  1.02 Ideal     SI2     J  2879
##  5  1.01 Ideal      I1     I  2896
##  6  1.02 Ideal      I1     I  2925
##  7  1.14 Ideal     SI1     J  3045
##  8  1.02 Ideal     SI2     H  3142
##  9  1.06 Ideal     SI2     I  3146
## 10  1.02 Ideal     VS2     I  3148
## # ... with 5,652 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This particular query returned just over 10% of the total data with 5,662 rows matching the conditions set. This feature is extremely important when dealing with database that house extremely large amounts of data. Having to query full tables would be extemely unfeasible in most situations.&lt;/p&gt;
&lt;p&gt;In addition to writing more complex sql queries, the &lt;code&gt;dbplyr&lt;/code&gt; package allows for R users to avoid having to write queries at all. This package allows users to create a reference to the sql table and interact with it using typical &lt;code&gt;dplyr&lt;/code&gt; verbs. We can recreate the query above using this method. First we will use the &lt;code&gt;tbl&lt;/code&gt; function to create the reference to the diamonds table in the database. Then we will be able to use that reference with all of our favorite &lt;code&gt;dplyr&lt;/code&gt; verbs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diamonds_tbl &amp;lt;- tbl(con, &amp;quot;diamonds&amp;quot;)

diamonds_tbl %&amp;gt;%
  select(carat, cut, clarity, color, price) %&amp;gt;%
  filter(carat &amp;gt; 1, 
         cut == &amp;quot;Ideal&amp;quot;) %&amp;gt;% 
  collect() %&amp;gt;%
  as.tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5,662 x 5
##    carat   cut clarity color price
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt;
##  1  1.01 Ideal      I1     I  2844
##  2  1.02 Ideal     SI2     H  2856
##  3  1.02 Ideal      I1     I  2872
##  4  1.02 Ideal     SI2     J  2879
##  5  1.01 Ideal      I1     I  2896
##  6  1.02 Ideal      I1     I  2925
##  7  1.14 Ideal     SI1     J  3045
##  8  1.02 Ideal     SI2     H  3142
##  9  1.06 Ideal     SI2     I  3146
## 10  1.02 Ideal     VS2     I  3148
## # ... with 5,652 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;collect&lt;/code&gt; verb is important if you want the full query to be brought into your R session. The &lt;code&gt;dbplyr&lt;/code&gt; package uses lazy evaluation and only brings in a portion of the query into your session.&lt;/p&gt;
&lt;p&gt;Let’s take a look at how these two methods compare using the &lt;code&gt;microbenchmark&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark::microbenchmark(
  db_query = dbGetQuery(con, query),
  dbplyr = diamonds_tbl %&amp;gt;%
             select(carat, cut, clarity, color, price) %&amp;gt;%
             filter(carat &amp;gt; 1, 
                    cut == &amp;quot;Ideal&amp;quot;) %&amp;gt;% 
             collect()
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##      expr      min       lq     mean   median       uq      max neval
##  db_query 10.10302 10.30996 10.63014 10.44201 10.60457 15.27123   100
##    dbplyr 51.23319 52.08525 54.66543 53.53378 54.32673 87.69649   100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, the &lt;code&gt;dbplyr&lt;/code&gt; method, while very familiar and potentially easier if you have no experience writing sql queries, takes nearly 6x as long as the straight sql query.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modify-tables-in-place&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modify Tables in Place&lt;/h1&gt;
&lt;p&gt;While there are a lot of blog posts and some great package vignettes about setting up your tables and querying sql databases, there is not too much (that I have seen) about modifying tables in place in your database. There are a few options that are possible when you want to modify a table in a sql database. The first option is to simply query the entire database, make your desired changes using your prefered R tools and then overwrite the table in the database. However, this approach is not practical if you have a large amount of data in your table.&lt;/p&gt;
&lt;p&gt;The method that I have found that seems to be fairly straight forward is using the &lt;code&gt;dbSendQuery&lt;/code&gt; function. While knowing this function is important, the more important part of this function is knowing what SQL commmads to include in your query. The blog post on &lt;a href=&#34;http://www.win-vector.com/blog/2016/02/using-postgresql-in-r/&#34;&gt;Win-Vector Blog&lt;/a&gt; concerning using PostgreSqL in R shows how you can drop entire tables from your database, and the &lt;code&gt;RSQLite&lt;/code&gt; vignette by Hadley Wickham shows how to delete rows that meet certain conditions. However, if you want to modify a table in your database, the sql commands needed are “update” “set”, and “where”. You can see below how we can use these commands.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;update_query &amp;lt;- paste(&amp;quot;update cars&amp;quot;,
                      &amp;quot;set mpg = 20&amp;quot;,
                      &amp;quot;where cyl = 6&amp;quot;)

dbSendQuery(con, update_query)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;SQLiteResult&amp;gt;
##   SQL  update cars set mpg = 20 where cyl = 6
##   ROWS Fetched: 0 [complete]
##        Changed: 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that 7 rows were changed in the database. Let’s now query the database and see how the table now looks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dbGetQuery(con, &amp;quot;select * from cars&amp;quot;) %&amp;gt;%
  as.tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 32 x 12
##              rowname   mpg   cyl  disp    hp  drat    wt  qsec    vs    am
##                &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1         Mazda RX4  20.0     6 160.0   110  3.90 2.620 16.46     0     1
##  2     Mazda RX4 Wag  20.0     6 160.0   110  3.90 2.875 17.02     0     1
##  3        Datsun 710  22.8     4 108.0    93  3.85 2.320 18.61     1     1
##  4    Hornet 4 Drive  20.0     6 258.0   110  3.08 3.215 19.44     1     0
##  5 Hornet Sportabout  18.7     8 360.0   175  3.15 3.440 17.02     0     0
##  6           Valiant  20.0     6 225.0   105  2.76 3.460 20.22     1     0
##  7        Duster 360  14.3     8 360.0   245  3.21 3.570 15.84     0     0
##  8         Merc 240D  24.4     4 146.7    62  3.69 3.190 20.00     1     0
##  9          Merc 230  22.8     4 140.8    95  3.92 3.150 22.90     1     0
## 10          Merc 280  20.0     6 167.6   123  3.92 3.440 18.30     1     0
## # ... with 22 more rows, and 2 more variables: gear &amp;lt;dbl&amp;gt;, carb &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like all of the rows where cyl = 6 have had their mpg changed to 20. While this is a somewhat trivial example, as you would most likely not want to change the results for a data set like this, this can be an incredibly useful feature if you are maintaining a database from R.&lt;/p&gt;
&lt;p&gt;You can modify more rows by adding additional arguments to the “set” command and add more conditions by setting additional arguments to the “where” command. For example, we can edit the diamonds table below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;update_query &amp;lt;- paste(&amp;quot;update diamonds&amp;quot;,
                      &amp;quot;set cut = &amp;#39;new Ideal&amp;#39;,&amp;quot;,
                      &amp;quot;color = &amp;#39;Z&amp;#39;&amp;quot;,
                      &amp;quot;where cut = &amp;#39;Ideal&amp;#39; and&amp;quot;,
                      &amp;quot;color = &amp;#39;E&amp;#39;&amp;quot;)
dbSendQuery(con, update_query)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;SQLiteResult&amp;gt;
##   SQL  update diamonds set cut = &amp;#39;new Ideal&amp;#39;, color = &amp;#39;Z&amp;#39; where cut = &amp;#39;Ideal&amp;#39; and color = &amp;#39;E&amp;#39;
##   ROWS Fetched: 0 [complete]
##        Changed: 3903&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that this changed 3903 rows in the diamonds dataset.&lt;/p&gt;
&lt;p&gt;Before we finish, it is imprtant to remember to disconnect from the in-memory database using the &lt;code&gt;dbDisconnect&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dbDisconnect(con)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;DBI&lt;/code&gt;, &lt;code&gt;odbc&lt;/code&gt;, and &lt;code&gt;dbplyr&lt;/code&gt; packages offer an incredible number of tools for interacting with SQL databases of all different kinds. While you are certainly able to navigate through most SQL query problems with only the functions provided in these packages, you can cartainly increase your capabilites by learning some basic SQL commands and how to use them in conjunction with the R functions provided in these packages.&lt;/p&gt;
&lt;/div&gt;
</description>
  </item>
  
<item>
  <title>Exploring English Premier League Historical Match Results</title>
  <link>/2017/08/10/epl-opening-weekend/</link>
  <pubDate>Thu, 10 Aug 2017 00:00:00 +0000</pubDate>
  
<guid>/2017/08/10/epl-opening-weekend/</guid>
  <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I was listening to &lt;a href=&#34;https://twitter.com/codinghorror&#34;&gt;Jeff Atwood’s&lt;/a&gt; interview on the podcast &lt;a href=&#34;http://developeronfire.com/&#34;&gt;Developer on Fire&lt;/a&gt; and he said something that struck home with me. It was along the lines of, “The best time to start blogging is yesterday.” I have been considering starting a blog about &lt;a href=&#34;https://twitter.com/hashtag/rstats&#34;&gt;#rstats&lt;/a&gt; but had been putting it off because of any number of reasons. But after listening to his interview, I decided now was as good of a time as any. With the help of Yihui Xie’s &lt;a href=&#34;https://github.com/rstudio/blogdown&#34;&gt;blogdown&lt;/a&gt;, I was able to set up the basic webpage pretty easily too.&lt;/p&gt;
&lt;p&gt;So In honor of the start of a new season for the English Premier League (EPL), I put together this exploratory data analysis of historical EPL data to see how teams typically do to start a season. I would love any feed back and suggestions! Please feel free to follow me on &lt;a href=&#34;twitter%20https://twitter.com/tb_goonR&#34;&gt;twitter&lt;/a&gt; (I just created it this past week to better keep up with #rstats rather than just continually googling it as I have been for the last year.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;general-data-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;General Data Analysis&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# loading in the required packages
suppressWarnings(suppressPackageStartupMessages({
  library(tidyr)
  library(ggplot2)
  library(lubridate)
  library(magrittr)
  library(tidyquant)
  library(purrr)
  library(ggjoy)
  library(dplyr)
}))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, we will load in the data. All of the data used in this analysis is from &amp;lt;www.football-data.co.uk&amp;gt;, and can be found on &lt;a href=&#34;https://github.com/tbradley1013/epl&#34;&gt;my github page&lt;/a&gt; under the epl repository.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;files &amp;lt;- list.files(path = &amp;quot;epl_results&amp;quot;, full.names = TRUE)

raw_data &amp;lt;- map(files, read.csv)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is the column information provided by the website:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;* Div = League Division
* Date = Match Date (dd/mm/yy)
* HomeTeam = Home Team
* AwayTeam = Away Team
* FTHG = Full Time Home Team Goals
* FTAG = Full Time Away Team Goals
* FTR = Full Time Result (H=Home Win, D=Draw, A=Away Win) 
* HTHG = Half Time Home Team Goals 
* HTAG = Half Time Away Team Goals 
* HTR = Half Time Result (H=Home Win, D=Draw, A=Away Win)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Match Statistics (where available):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;* Attendance = Crowd Attendance
* Referee = Match Referee
* HS = Home Team Shots
* AS = Away Team Shots
* HST = Home Team Shots on Target
* AST = Away Team Shots on Target
* HHW = Home Team Hit Woodwork
* AHW = Away Team Hit Woodwork
* HC = Home Team Corners
* AC = Away Team Corners
* HF = Home Team Fouls Committed
* AF = Away Team Fouls Committed
* HO = Home Team Offsides
* AO = Away Team Offsides
* HY = Home Team Yellow Cards
* AY = Away Team Yellow Cards
* HR = Home Team Red Cards
* AR = Away Team Red Cards&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are more columns provided in the raw data set that have to do with betting odds, however, we will remove them as they are not going to be used in this analysis. Additionally, only two seasons data has attendance recorded, so this will be removed. Looking through the data, all of the data sets for the 2000/2001 through the 2016/2017 seasons have all of the match statistics listed above. The data sets prior to the 2000/2001 data set only have the general data and none of the match statistics. Because of these differences in available data we will only use the general statistics for the first part of this analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_general &amp;lt;- map(raw_data, function(x){
  output &amp;lt;- x %&amp;gt;%
    filter(Date != &amp;quot;&amp;quot;) %&amp;gt;%  #the csv files pulled in some extra rows, this line removes them
    mutate(Date = dmy(Date)) %&amp;gt;% #converting the Date column from a factor to date object
    select(Div:HTR) %&amp;gt;%
    mutate(season = ifelse(month(Date) &amp;gt; 7,
                                   year(Date) + 1,
                                    year(Date)))  #creating a reference date 
  return(output)
})

data &amp;lt;- do.call(rbind, data_general) %&amp;gt;% as.tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8,360 x 11
##       Div       Date    HomeTeam       AwayTeam  FTHG  FTAG    FTR  HTHG
##    &amp;lt;fctr&amp;gt;     &amp;lt;date&amp;gt;      &amp;lt;fctr&amp;gt;         &amp;lt;fctr&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;fctr&amp;gt; &amp;lt;int&amp;gt;
##  1     E0 1995-08-19 Aston Villa     Man United     3     1      H     3
##  2     E0 1995-08-19   Blackburn            QPR     1     0      H     1
##  3     E0 1995-08-19     Chelsea        Everton     0     0      D     0
##  4     E0 1995-08-19   Liverpool Sheffield Weds     1     0      H     0
##  5     E0 1995-08-19    Man City      Tottenham     1     1      D     0
##  6     E0 1995-08-19   Newcastle       Coventry     3     0      H     1
##  7     E0 1995-08-19 Southampton  Nott&amp;#39;m Forest     3     4      A     1
##  8     E0 1995-08-19    West Ham          Leeds     1     2      A     1
##  9     E0 1995-08-19   Wimbledon         Bolton     3     2      H     2
## 10     E0 1995-08-20     Arsenal  Middlesbrough     1     1      D     1
## # ... with 8,350 more rows, and 3 more variables: HTAG &amp;lt;int&amp;gt;, HTR &amp;lt;fctr&amp;gt;,
## #   season &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the data from the csv files is all in one data frame, we can do some manipulation to get it into a more tidy format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_tidy &amp;lt;- data %&amp;gt;%
  gather(key = &amp;quot;venue&amp;quot;, value = team, HomeTeam:AwayTeam) %&amp;gt;% 
  arrange(Date) %&amp;gt;%
  mutate_if(is.factor, as.character) %&amp;gt;%
  mutate(venue = ifelse(venue == &amp;quot;HomeTeam&amp;quot;,
                        &amp;quot;Home&amp;quot;,
                        &amp;quot;Away&amp;quot;),
         FTR = case_when(venue == &amp;quot;Home&amp;quot; &amp;amp; FTR == &amp;quot;H&amp;quot; ~ &amp;quot;W&amp;quot;,
                         venue == &amp;quot;Home&amp;quot; &amp;amp; FTR == &amp;quot;A&amp;quot; ~ &amp;quot;L&amp;quot;,
                         venue == &amp;quot;Away&amp;quot; &amp;amp; FTR == &amp;quot;H&amp;quot; ~ &amp;quot;L&amp;quot;,
                         venue == &amp;quot;Away&amp;quot; &amp;amp; FTR == &amp;quot;A&amp;quot; ~ &amp;quot;W&amp;quot;,
                         TRUE ~ FTR),
         HTR = case_when(venue == &amp;quot;Home&amp;quot; &amp;amp; HTR == &amp;quot;H&amp;quot; ~ &amp;quot;W&amp;quot;,
                         venue == &amp;quot;Home&amp;quot; &amp;amp; HTR == &amp;quot;A&amp;quot; ~ &amp;quot;L&amp;quot;,
                         venue == &amp;quot;Away&amp;quot; &amp;amp; HTR == &amp;quot;H&amp;quot; ~ &amp;quot;L&amp;quot;,
                         venue == &amp;quot;Away&amp;quot; &amp;amp; HTR == &amp;quot;A&amp;quot; ~ &amp;quot;W&amp;quot;,
                         TRUE ~ HTR),
         FTGF = ifelse(venue == &amp;quot;Home&amp;quot;, FTHG, FTAG),  #Full Time Goals For
         FTGA = ifelse(venue == &amp;quot;Home&amp;quot;, FTAG, FTHG),  #Full Time Goals Against
         HTGF = ifelse(venue == &amp;quot;Home&amp;quot;, HTHG, HTAG),  #Half Time Goals For
         HTGA = ifelse(venue == &amp;quot;Home&amp;quot;, HTAG, HTHG),  #Half Time Goals Against
         goal_diff = FTGF - FTGA,                    #goal difference
         points_earned = case_when(FTR == &amp;quot;W&amp;quot; ~ 3,           #adding points
                                   FTR == &amp;quot;D&amp;quot; ~ 1,
                                   FTR == &amp;quot;L&amp;quot; ~ 0)) %&amp;gt;% 
  select(Div, season, Date, team, venue, FTR, FTGF, 
         FTGA, HTR, HTGF, HTGA, goal_diff, points_earned) %&amp;gt;%
  group_by(season, team) %&amp;gt;%
  mutate(points = cumsum(points_earned),
         goal_diff_tot = cumsum(goal_diff)) %&amp;gt;% #calculating the number of points each team has through out the season
  ungroup()

data_tidy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 16,720 x 15
##      Div season       Date        team venue   FTR  FTGF  FTGA   HTR  HTGF
##    &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;date&amp;gt;       &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt;
##  1    E0   1996 1995-08-19 Aston Villa  Home     W     3     1     W     3
##  2    E0   1996 1995-08-19   Blackburn  Home     W     1     0     W     1
##  3    E0   1996 1995-08-19     Chelsea  Home     D     0     0     D     0
##  4    E0   1996 1995-08-19   Liverpool  Home     W     1     0     D     0
##  5    E0   1996 1995-08-19    Man City  Home     D     1     1     L     0
##  6    E0   1996 1995-08-19   Newcastle  Home     W     3     0     W     1
##  7    E0   1996 1995-08-19 Southampton  Home     L     3     4     L     1
##  8    E0   1996 1995-08-19    West Ham  Home     L     1     2     W     1
##  9    E0   1996 1995-08-19   Wimbledon  Home     W     3     2     D     2
## 10    E0   1996 1995-08-19  Man United  Away     L     1     3     L     0
## # ... with 16,710 more rows, and 5 more variables: HTGA &amp;lt;int&amp;gt;,
## #   goal_diff &amp;lt;int&amp;gt;, points_earned &amp;lt;dbl&amp;gt;, points &amp;lt;dbl&amp;gt;,
## #   goal_diff_tot &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To ensure that our tidying did not create any missing values, we can use the &lt;code&gt;summarise_all()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_tidy %&amp;gt;% 
  summarise_all(function(x) sum(is.na(x)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 15
##     Div season  Date  team venue   FTR  FTGF  FTGA   HTR  HTGF  HTGA
##   &amp;lt;int&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
## 1     0      0     0     0     0     0     0     0     0     0     0
## # ... with 4 more variables: goal_diff &amp;lt;int&amp;gt;, points_earned &amp;lt;int&amp;gt;,
## #   points &amp;lt;int&amp;gt;, goal_diff_tot &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we know that the data is in a tidy format, we can begin exploring the data. As an Arsenal fan, I think we should start by looking at how Arsenal has done each year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_tidy %&amp;gt;%
  filter(team == &amp;quot;Arsenal&amp;quot;,
         season &amp;lt; 2007) %&amp;gt;%
  ggplot(aes(Date, points)) +
  facet_wrap(~season, scales = &amp;quot;free&amp;quot;) +
  geom_line() +
  theme_tq() +
  scale_x_date(date_breaks = &amp;quot;2 month&amp;quot;, date_labels = &amp;quot;%m/%d&amp;quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, 
                                   vjust = 0.5, size = 12),
        axis.text.y = element_text(size = 12),
        strip.text = element_text(size = 12),
        axis.title = element_text(size = 12)) +
  labs(x = &amp;quot;&amp;quot;, 
       y = &amp;quot;Point Tally&amp;quot;,
       title = &amp;quot;Arsenal Point Tally over the course of each season from\nthe 1995/1996 season to the 2005/2006 season&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-08-09-exploring-english-premier-league-historical-match-results_files/figure-html/arsenal-points-graph-1-1.png&#34; width=&#34;800px&#34; height=&#34;600px&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_tidy %&amp;gt;%
  filter(team == &amp;quot;Arsenal&amp;quot;,
         season &amp;gt;= 2007) %&amp;gt;%
  ggplot(aes(Date, points)) +
  facet_wrap(~season, scales = &amp;quot;free&amp;quot;) +
  geom_line() +
  theme_tq() +
  scale_x_date(date_breaks = &amp;quot;2 month&amp;quot;, 
               date_labels = &amp;quot;%m/%d&amp;quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, 
                                   vjust = 0.5, size = 12),
        axis.text.y = element_text(size = 12),
        strip.text = element_text(size = 12),
        axis.title = element_text(size = 12))+
  labs(x = &amp;quot;&amp;quot;, 
       y = &amp;quot;Point Tally&amp;quot;,
       title = &amp;quot;Arsenal Point Tally over the course of each season from\nthe 2006/2007 season to the 2016/2017 season&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-08-09-exploring-english-premier-league-historical-match-results_files/figure-html/arsenal-points-graph-2-1.png&#34; width=&#34;800px&#34; height=&#34;600px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From these graphs, it looks like Arsenal follow a similar pattern every year, which is not surprising since the 2017 season was the first in Arsene Wenger’s tenure that they have not finished in the top 4. Looking at the plot for the 2017 season, it is clear that the period of the season that killed their chances of finishing in the top 4 was the stretch of games between February and March where their point increase flat lined. Let’s now take a look at team’s average finishing point total.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;season_ending &amp;lt;- data_tidy %&amp;gt;%
  group_by(season, team) %&amp;gt;%
  summarise(final_points = max(points), 
            final_goals_for = sum(FTGF), 
            final_goals_against = sum(FTGA),
            final_goal_diff = sum(goal_diff)) %&amp;gt;%
  ungroup() %&amp;gt;%
  group_by(season) %&amp;gt;%
  mutate(table_position = 20 - rank(final_points, ties.method = &amp;quot;min&amp;quot;)) %&amp;gt;%
  ungroup()
  
season_ending&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 440 x 7
##    season        team final_points final_goals_for final_goals_against
##     &amp;lt;dbl&amp;gt;       &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;           &amp;lt;int&amp;gt;               &amp;lt;int&amp;gt;
##  1   1996     Arsenal           63              49                  32
##  2   1996 Aston Villa           63              52                  35
##  3   1996   Blackburn           61              61                  47
##  4   1996      Bolton           29              39                  71
##  5   1996     Chelsea           50              46                  44
##  6   1996    Coventry           38              42                  60
##  7   1996     Everton           61              64                  44
##  8   1996       Leeds           43              40                  57
##  9   1996   Liverpool           71              70                  34
## 10   1996    Man City           38              33                  58
## # ... with 430 more rows, and 2 more variables: final_goal_diff &amp;lt;int&amp;gt;,
## #   table_position &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From this output, right away we can see that point ties result in the rank function returning a tie. The EPL determines ties first by highest goal difference, and then goals for. Looking at this first example of the tie between Arsenal and Aston Villa, we can see that they both had a +17 overall goal difference but Aston Villa had 3 more goals for, meaning they finished 4th and Arsenal finished 5th. For now, we won’t worry about fixing these ties.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;season_ending_stats &amp;lt;- season_ending %&amp;gt;%
  group_by(team) %&amp;gt;%
  summarise(avg_final_points = round(mean(final_points), 0), 
            sd_final_points = round(sd(final_points), 0), 
            avg_final_goals_for = round(mean(final_goals_for), 0), 
            sd_final_goals_for = round(sd(final_goals_for), 0),
            avg_final_goals_against = round(mean(final_goals_against), 0), 
            sd_final_goals_against = round(sd(final_goals_against), 0),
            avg_table_position = round(mean(table_position), 0),
            sd_table_poisiton = round(sd(table_position), 0),
            num_seasons = n())

season_ending_stats&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 46 x 10
##           team avg_final_points sd_final_points avg_final_goals_for
##          &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;               &amp;lt;dbl&amp;gt;
##  1     Arsenal               75               7                  71
##  2 Aston Villa               50              11                  46
##  3    Barnsley               35             NaN                  37
##  4  Birmingham               43               7                  39
##  5   Blackburn               48              10                  48
##  6   Blackpool               39             NaN                  55
##  7      Bolton               44               9                  44
##  8 Bournemouth               44               3                  50
##  9    Bradford               31               7                  34
## 10     Burnley               34               5                  36
## # ... with 36 more rows, and 6 more variables: sd_final_goals_for &amp;lt;dbl&amp;gt;,
## #   avg_final_goals_against &amp;lt;dbl&amp;gt;, sd_final_goals_against &amp;lt;dbl&amp;gt;,
## #   avg_table_position &amp;lt;dbl&amp;gt;, sd_table_poisiton &amp;lt;dbl&amp;gt;, num_seasons &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see from the tibble summary, there are several teams that have only been in the premier league for a single year, resulting in NaN values for the standard deviation columns. To avoid cluttering the next graph, we will remove any teams that have not been in the premier league for at least 10 of the 22 seasons being analyzed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;season_ending_stats %&amp;gt;%
  filter(num_seasons &amp;gt;= 10) %&amp;gt;%
  arrange(-avg_final_points) %&amp;gt;%
  mutate(team = factor(team, team)) %&amp;gt;%
  ggplot(aes(team, avg_final_points)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, fill = &amp;quot;red&amp;quot;) +
  geom_point(color = &amp;quot;navy&amp;quot;) +
  geom_errorbar(aes(ymin = avg_final_points - 2*sd_final_points, 
                    ymax = avg_final_points + 2*sd_final_points), 
                color = &amp;quot;navy&amp;quot;, size = 1) +
  theme_tq() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, 
                                   vjust = 0.5, size = 12),
        axis.text.y = element_text(size = 12),
        axis.title = element_text(size = 12)) +
  labs(x = &amp;quot;Team&amp;quot;,
       y = &amp;quot;Final Points&amp;quot;,
       title = &amp;quot;Average Final Points from 1995/1996 season to 2016/2017 season&amp;quot;,
       subtitle = &amp;quot;Only teams with 10 or more seasons in the EPL were included&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-08-09-exploring-english-premier-league-historical-match-results_files/figure-html/bar-graph-season-stats-1.png&#34; width=&#34;800px&#34; height=&#34;600px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like Manchester United have the highest average final points per season. They are followed closely by Arsenal and Chelsea, who are then closely followed by Liverpool, Manchester City, and Tottenham. It is worth noting that Chelsea and Manchester City both have much higher error associated with their mean value, indicating they have much more fluctuation in the final point tallies. Another interesting take away from this figure is that the lowest average final point tally for teams that have been in the premier league for at least 10 of the previous 22 seasons is 39 points. 39 points typically guarantees a season that is safe from relegation. However, for the teams with the lower average finally point tally, such as Sunderland and West Brom, see the lower tails of their error bars treading dangerously close to relegation zone.&lt;/p&gt;
&lt;p&gt;Now, it will be interesting to see how these teams final point tallies will look on the new joyplots from the &lt;code&gt;ggjoy&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;season_ending %&amp;gt;%
  group_by(team) %&amp;gt;%
  mutate(num_seasons = n(),
         avg_final_points = round(mean(final_points), 0)) %&amp;gt;% #add these columns for filtering and factoring respectively
  ungroup() %&amp;gt;%
  filter(num_seasons &amp;gt;= 10) %&amp;gt;%
  arrange(avg_final_points) %&amp;gt;%
  mutate(team = factor(team, unique(team))) %&amp;gt;%
  ggplot(aes(final_points, team)) +
  geom_joy(scale = 0.9, rel_min_height = 0.01, 
           fill = &amp;quot;red&amp;quot;, color = &amp;quot;black&amp;quot;, size = 1) +
  theme_tq()+
  theme(axis.text = element_text(size = 12),
        axis.title = element_text(size = 12)) +
  labs(x = &amp;quot;Final Points&amp;quot;,
       y = &amp;quot;Team&amp;quot;,
       title = &amp;quot;Joyplot showing EPL teams&amp;#39; individual season final point tally distribution&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-08-09-exploring-english-premier-league-historical-match-results_files/figure-html/final-points-joyplot-1.png&#34; width=&#34;800px&#34; height=&#34;600px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The joyplot confirms the conclusions drawn from the bar chart. Manchester United and Arsenal have the highest final point distribution and both have close to a bell curve. However, Chelsea and Manchester City both have much wider distributions as they have much more ups and downs over the past 22 years.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;looking-at-opening-weekend-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Looking at opening weekend results&lt;/h1&gt;
&lt;p&gt;Since this weekend marks the start of the 2017/2018 season, it will be interesting to see how teams have fared over the last 22 seasons. First, we will start by filtering for the first game of the season.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;opening_games &amp;lt;- data_tidy %&amp;gt;%
  group_by(season, team) %&amp;gt;%
  mutate(final_points = max(points), 
            final_goals_for = sum(FTGF), 
            final_goals_against = sum(FTGA),
         game = rank(Date)) %&amp;gt;%
  ungroup() %&amp;gt;%
  group_by(season, game) %&amp;gt;%
  mutate(table_position = 21 - min_rank(final_points)) %&amp;gt;%
  ungroup() %&amp;gt;%
  filter(game == 1) %&amp;gt;%
  group_by(team) %&amp;gt;%
  mutate(final_points_last = lag(final_points),
         final_goals_for_last = lag(final_goals_for),
         final_goals_against_last = lag(final_goals_against),
         final_table_position_last = lag(table_position)) %&amp;gt;%
  ungroup()

opening_games&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 440 x 24
##      Div season       Date        team venue   FTR  FTGF  FTGA   HTR  HTGF
##    &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;date&amp;gt;       &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt;
##  1    E0   1996 1995-08-19 Aston Villa  Home     W     3     1     W     3
##  2    E0   1996 1995-08-19   Blackburn  Home     W     1     0     W     1
##  3    E0   1996 1995-08-19     Chelsea  Home     D     0     0     D     0
##  4    E0   1996 1995-08-19   Liverpool  Home     W     1     0     D     0
##  5    E0   1996 1995-08-19    Man City  Home     D     1     1     L     0
##  6    E0   1996 1995-08-19   Newcastle  Home     W     3     0     W     1
##  7    E0   1996 1995-08-19 Southampton  Home     L     3     4     L     1
##  8    E0   1996 1995-08-19    West Ham  Home     L     1     2     W     1
##  9    E0   1996 1995-08-19   Wimbledon  Home     W     3     2     D     2
## 10    E0   1996 1995-08-19  Man United  Away     L     1     3     L     0
## # ... with 430 more rows, and 14 more variables: HTGA &amp;lt;int&amp;gt;,
## #   goal_diff &amp;lt;int&amp;gt;, points_earned &amp;lt;dbl&amp;gt;, points &amp;lt;dbl&amp;gt;,
## #   goal_diff_tot &amp;lt;int&amp;gt;, final_points &amp;lt;dbl&amp;gt;, final_goals_for &amp;lt;int&amp;gt;,
## #   final_goals_against &amp;lt;int&amp;gt;, game &amp;lt;dbl&amp;gt;, table_position &amp;lt;dbl&amp;gt;,
## #   final_points_last &amp;lt;dbl&amp;gt;, final_goals_for_last &amp;lt;int&amp;gt;,
## #   final_goals_against_last &amp;lt;int&amp;gt;, final_table_position_last &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have a data set with all of the teams opening games results, we can look at some of the factors that could influence opening weekend results. First let’s look at how home field advantage helps teams on opening weekend&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;opening_home_adv &amp;lt;- opening_games %&amp;gt;%
  group_by(venue) %&amp;gt;%
  summarise(perc_W = round((sum(FTR == &amp;quot;W&amp;quot;)/n()) * 100, 1), 
            perc_L = round((sum(FTR == &amp;quot;L&amp;quot;)/n()) * 100, 1),
            perc_D = round((sum(FTR == &amp;quot;D&amp;quot;)/n()) * 100, 1))

opening_home_adv&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 4
##   venue perc_W perc_L perc_D
##   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1  Away   30.5   42.3   27.3
## 2  Home   41.8   30.9   27.3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like the home teams have better luck on the first weekend of the season than the away team. Let’s look at home this compares with the home field advantage of all games to see if there is any difference for the first week.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;home_adv &amp;lt;- data_tidy %&amp;gt;%
  group_by(venue) %&amp;gt;%
  summarise(perc_W = round((sum(FTR == &amp;quot;W&amp;quot;)/n()) * 100, 1), 
            perc_L = round((sum(FTR == &amp;quot;L&amp;quot;)/n()) * 100, 1),
            perc_D = round((sum(FTR == &amp;quot;D&amp;quot;)/n()) * 100, 1))

home_adv&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 4
##   venue perc_W perc_L perc_D
##   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1  Away   27.5   46.5     26
## 2  Home   46.5   27.5     26&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like the away team actually fairs slightly better on opening weekend than during the rest of the season. Let’s now take a look at the winning percentage by year for the home teams and the away teams.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;opening_games %&amp;gt;%
  group_by(season, venue) %&amp;gt;%
  summarise(perc_W = round((sum(FTR == &amp;quot;W&amp;quot;)/n()) * 100, 1), 
            perc_L = round((sum(FTR == &amp;quot;L&amp;quot;)/n()) * 100, 1),
            perc_D = round((sum(FTR == &amp;quot;D&amp;quot;)/n()) * 100, 1)) %&amp;gt;%
  gather(key = &amp;quot;result&amp;quot;, value = &amp;quot;percentage&amp;quot;, perc_W:perc_D) %&amp;gt;%
  mutate(result = case_when(result == &amp;quot;perc_W&amp;quot; ~ &amp;quot;Winning Percentage&amp;quot;,
                            result == &amp;quot;perc_L&amp;quot; ~ &amp;quot;Losing Percentage&amp;quot;,
                            TRUE ~ &amp;quot;Percent Draws&amp;quot;)) %&amp;gt;%
  filter(venue == &amp;quot;Home&amp;quot;) %&amp;gt;%
  ggplot(aes(season, percentage, color = result)) +
  geom_point() +
  geom_line(size = 1) +
  theme_tq() +
  scale_color_tq() +
  theme(axis.text.x = element_text(angle = 90, 
                                   vjust = 0.5, 
                                   hjust = 1, 
                                   size = 12),
        axis.text.y = element_text(size = 12),
        legend.title = element_blank()) +
  labs(x = &amp;quot;Year&amp;quot;,
       y = &amp;quot;Percentage of Results by Year&amp;quot;,
       title = &amp;quot;The Percent of Results (Win, Draw, Loss) per Season\nfor Home Teams on Opening Weekend&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-08-09-exploring-english-premier-league-historical-match-results_files/figure-html/home-adv-graph-1.png&#34; width=&#34;800px&#34; height=&#34;600px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like the winning percentage for home teams fluctuates and there is no clear trend as to whether being home on opening weekend provides any benefit. Since 2014, the home teams have a higher losing percentage than winning percentage. Perhaps, in recent years, there has been some shift that has made playing away from home more desirable on opening weekend. Likely this is just random, however, and there is no overall benefit to being home or away. While there does not appear to be any overall inferences that can be made from playing home or away on opening weekend, looking at the winning percentage for home and away opening games for the individual teams may provide some interesting results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;home_adv_team &amp;lt;- opening_games %&amp;gt;%
  group_by(team) %&amp;gt;%
  #removing teams that have not played at least ten seasons in the premier league
  mutate(num_seasons = n()) %&amp;gt;%
  filter(num_seasons &amp;gt;= 10) %&amp;gt;% 
  select(-num_seasons) %&amp;gt;%
  ungroup() %&amp;gt;%
  group_by(team, venue) %&amp;gt;%
  summarise(winning_percentage = round((sum(FTR == &amp;quot;W&amp;quot;)/n()) * 100, 0), 
            num_games_total = n()) %&amp;gt;%
  ungroup()

home_adv_team&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 36 x 4
##           team venue winning_percentage num_games_total
##          &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;           &amp;lt;int&amp;gt;
##  1     Arsenal  Away                 43               7
##  2     Arsenal  Home                 60              15
##  3 Aston Villa  Away                 31              13
##  4 Aston Villa  Home                 50               8
##  5   Blackburn  Away                 40               5
##  6   Blackburn  Home                 40              10
##  7      Bolton  Away                 43               7
##  8      Bolton  Home                 50               6
##  9     Chelsea  Away                 56               9
## 10     Chelsea  Home                 77              13
## # ... with 26 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;home_adv_team %&amp;gt;%
  arrange(-winning_percentage) %&amp;gt;%
  mutate(team = factor(team, unique(team)),
         venue = factor(venue, levels = c(&amp;quot;Home&amp;quot;, &amp;quot;Away&amp;quot;))) %&amp;gt;%
  ggplot(aes(team, winning_percentage, fill = venue)) +
  facet_wrap(~venue, scales = &amp;quot;fixed&amp;quot;, ncol = 1) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) +
  theme_tq() +
  scale_fill_tq() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 12),
        axis.text.y = element_text(size = 12),
        strip.text = element_text(size = 12),
        legend.position = &amp;quot;none&amp;quot;) +
  geom_text(aes(x = team, y = winning_percentage + 5, label = paste0(&amp;quot;n=&amp;quot;, as.character(num_games_total)))) +
  labs(x = &amp;quot;&amp;quot;,
       y = &amp;quot;Winning Percentage&amp;quot;,
       title = &amp;quot;Winning Percentage for Teams on Opening Weekend both Home and Away&amp;quot;,
       subtitle = &amp;quot;Only teams with more than 10 seasons in the premier league were included&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-08-09-exploring-english-premier-league-historical-match-results_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;800px&#34; height=&#34;600px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This figure seems to provide much more insight into how home field advantage can impact teams. For example, Arsenal have a 60% winning percentage at home on opening weekend, and their opponent this weekend, Leicester City, have not won a opening game away from home in the premier league since 1996. This is a very positive sign for the gunners as their match is being played at the Emirates Stadium (Arsenal’s Home Field). Chelsea have a winning percentage in the high 70s when they play their opening game at home, as they do this year. That fact seems promising (unfortunately) for the blues, as their opening match this year is at home. Other noticeable impacts of home field advantage are New Castle United and Sunderland. New Castle have a home winning percentage of 50% on opening weekend, but only have a 10% winning percentage for away games on opening weekend. Fortunately for them, they are also playing at home this year. Unfortunately for them, they are matched up with West Ham who don’t have much issue playing on the road opening weekend, with a 40% winning percentage when away from home. Sunderland, similarly has an average home winning percentage of just under 40% for opening weekend, but have a winning percentage of only just north of 10% when playing away from home. However, Sunderland were relegated last season, so maybe they will have better luck in the Championship. Several teams seem not to be impacted by whether or not they play at home on opening weekend. Manchester United and Manchester City both only see a slight dip in winning percentage when away from home, and Blackburn has a 40% winning percentage when both home and away on opening weekend.&lt;/p&gt;
&lt;p&gt;Let’s take a look at each team’s overall winning percentage on opening weekend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;opening_games %&amp;gt;%
  group_by(team) %&amp;gt;%
  #removing teams that have not played at least ten seasons in the premier league
  mutate(num_seasons = n()) %&amp;gt;%
  filter(num_seasons &amp;gt;= 10) %&amp;gt;% 
  select(-num_seasons) %&amp;gt;%
  ungroup() %&amp;gt;%
  group_by(team) %&amp;gt;%
  summarise(winning_percentage = round((sum(FTR == &amp;quot;W&amp;quot;)/n()) * 100, 0), 
            num_games_total = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  arrange(-winning_percentage) %&amp;gt;%
  mutate(team = factor(team, unique(team))) %&amp;gt;%
  ggplot(aes(team, winning_percentage)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, fill = &amp;quot;red&amp;quot;, color = &amp;quot;black&amp;quot;) +
  theme_tq() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 12),
        axis.text.y = element_text(size = 12),
        strip.text = element_text(size = 12),
        legend.position = &amp;quot;none&amp;quot;) +
  geom_text(aes(x = team, y = winning_percentage + 5, label = paste0(&amp;quot;n=&amp;quot;, as.character(num_games_total)))) +
  labs(x = &amp;quot;&amp;quot;,
       y = &amp;quot;Winning Percentage&amp;quot;,
       title = &amp;quot;Winning Percentage for Teams on Opening Weekend&amp;quot;,
       subtitle = &amp;quot;Only teams with more than 10 seasons in the premier league were included&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-08-09-exploring-english-premier-league-historical-match-results_files/figure-html/opening-win-perc-1.png&#34; width=&#34;800px&#34; height=&#34;600px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like the results follow a similar pattern to the previous figure, as expected. Chelsea and Manchester United have the best winning percentages (&amp;gt;60%) on opening weekend, with Arsenal, Liverpool, Manchester City, and West Ham all close behind in the 50% range.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;It looks like the odds are in favor of several of the typical powerhouse teams to perform well this weekend, if history has anything to say about it. There is much more that can be done with this data set and I hope to revisit it at a later date. I hope you all enjoyed reading this, I hope to put out more blog posts in the future!&lt;/p&gt;
&lt;p&gt;And of course, COYG!!&lt;/p&gt;
&lt;/div&gt;
</description>
  </item>
  
<item>
  <title>Presentations</title>
  <link>/presentations/</link>
  <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
  
<guid>/presentations/</guid>
  <description>&lt;p&gt;&lt;strong&gt;Comparing Illumina MiSeq and PacBio SMRT Sequencing of Fecal Samples from Various Animal Sources Potentially Contributing to Microbial Contamination of the Delaware River Watershed&lt;/strong&gt;&lt;br/&gt;
&lt;em&gt;Delaware Watershed Research Conferece 2018&lt;/em&gt;&lt;br/&gt;
Philadelphia, PA - November 2018&lt;br/&gt;
&lt;strong&gt;Abstract&lt;/strong&gt;&lt;br/&gt;
Next generation sequencing technologies allow for vast amounts of information to be collected about microbial communities in order to better understand their structure and function. There are several sequencing technologies available that allow for different amounts of DNA or RNA to be sequenced. The different approaches and chemistry used by sequencing instrument manufacturers results in large differences in total sequencing yield, read length limitations, and sequence data accuracy. For example, a typical run on one of the most popular sequencing technologies, Illumina MiSeq, may produce 25 million high quality paired end 300 bp reads (2 x 300 bp). Conversely, the new Pacific Biosciences (PacBio) Sequel platform can produce reads of significantly longer lengths (averaging 10-14 kb) at a sacrifice in total yield (~ 365,000 reads) and higher error rates.&lt;/p&gt;

&lt;p&gt;Investigators researching microbiomes are faced with the challenge of selecting between technologies and chemistry to maximize the utility of the sequencing data. This can be particularly difficult as significant tradeoffs exist between the additional information that may be contained in longer reads or the negative effects of higher error rates or differences in sequencing depth, as well as their ultimate costs. To investigate these effects, these two technologies were employed to sequence 32 fecal samples targeting the 16S rRNA gene of bacteria. The Illumina MiSeq run targeted the V4-V5 hyper-variable regions (~300-350 bp) while the PacBio Sequel run targeted the full length 16S rRNA gene (~1500 bp). This study examines how closely the taxonomic assignments for these different technologies matched. It was investigated whether restricting sequence length to specific hyper-variable regions resulted in misclassification or overconfidence in taxonomic classification. Overall, both of these technologies resulted in similar taxonomic assignments with 46.4% of matches OTU&amp;rsquo;s being classified identically to the genus level. Only 1.9% of MiSeq OTUs were matched with a SMRT OTU where both OTUs were assigned differently at the genus level. In addition, these fecal samples, from 9 different animal sources, were used in conjunction with water samples taken from waterways in the Delaware watershed to see what impact on river microbiome these different animals may have. Differential abundances were used to identify genera that are unique to specific animals, in order to determine how these sources may be affecting river water quality. The results from this study will allow us to determine the pros and cons of using short-read vs. long-read sequencing for microbial community analyses and for microbial source tracking.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Developing a New Approach for Proactive Nitrification Monitoring in a Chloraminated Drinking Water Distribution System&lt;/strong&gt;&lt;br/&gt;
&lt;em&gt;AWWA Water Quality Technology Conference&lt;/em&gt;&lt;br/&gt;
Toronto, ON, Canada - November 2018&lt;br/&gt;
&lt;strong&gt;Abstract&lt;/strong&gt;&lt;br/&gt;
This abstract presents a proactive approach to managing nitrification in a chloraminated drinking water distribution system. This deals with an issue that is relevant in most drinking water systems that use chloramines as a primary or secondary disinfectant. The authors will present ways in which Philadelphia are implementing these proactive approaches.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://apps.nelac-institute.org/nemc/2016/docs/presentations/Thu-Use%20of%20Continuous%20Monitoring%20for%20possible%20uses%20in%20Compliance%20Monitoring-21.6-Bradley.pdf&#34;&gt;&lt;strong&gt;Effective Use of Real-Time Water Quality Data&lt;/strong&gt;&lt;/a&gt;&lt;br/&gt;
&lt;em&gt;National Environmental Monitoring Conference&lt;/em&gt;&lt;br/&gt;
Orange County, CA - August 2016&lt;br/&gt;
&lt;strong&gt;Abstract&lt;/strong&gt;&lt;br/&gt;
Online sensor data, whether in drinking water production and delivery or elsewhere, require significant care and handling. Required care and handling of the data are dictated by the sensor precision and accuracy, the intended use of the sensor (event detection, process control, research, compliance monitoring or other purposes), and the sensor context (their placement in the water production system, the variability in a monitored parameter at the sensor location, the difference between average conditions at a particular sensor location and a regulatory level or level of concern). This study presents an analysis of real-time water quality monitoring data management and analysis for sensors deployed in Philadelphia Water’s distribution system and used for event detection. First, the data collection, management and analysis tools are described and difficulties encountered and overcome while developing the system are highlighted. Those difficulties include both the complexity of collecting large volumes of data from a dispersed network of sensors and steps required to ensure data conform to requirements of event detection tools. Second, analyses are presented that quantify the required precision and accuracy of sensors deployed in the Philadelphia Water distribution system. Analyses include assessment in the variability of water quality data at several time scales and establishment of sensor performance metrics consistent with their use as part of an early warning system. In general, sensors in current use in Philadelphia Water’s sensor network were shown to meet required precision and accuracy for use as components in an event detection system. As shown in a companion paper to this paper, meeting the requirements was only possible after significant efforts to improve sensor installations and establish a rigorous operations and maintenance (O&amp;amp;M) effort. Finally, analyses for establishing sensor control limits are presented. Control limits are a critical input to event detection algorithms. To establish control limits, long time series of water quality data were analyzed and variability in the observations at the global, seasonal and daily time scales was characterized. Results indicate that control limits that optimize event detection performance (maximize detection of true events and minimize false positive and false negative detections) vary seasonally or possibly at shorter time scales.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;
&lt;br/&gt;&lt;/p&gt;
</description>
  </item>
  
<item>
  <title>Publications</title>
  <link>/publications/</link>
  <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
  
<guid>/publications/</guid>
  <description>&lt;p&gt;&lt;a href=&#34;https://awwa.onlinelibrary.wiley.com/doi/abs/10.1002/awwa.1143&#34;&gt;&lt;strong&gt;Using Historical LCR and Water Quality Data to Evaluate Corrosion Control Treatment&lt;/strong&gt;&lt;/a&gt;&lt;br/&gt;
&lt;em&gt;Journal - American Water Works Association&lt;/em&gt; - October 2018&lt;br/&gt;
Tyler Bradley and Nicola Horscroft&lt;br/&gt;
&lt;strong&gt;DOI:&lt;/strong&gt; 10.1002/awwa.1143&lt;br/&gt;
&lt;strong&gt;Abstract&lt;/strong&gt;&lt;br/&gt;
Historical Lead and Copper Rule (LCR) regulatory sampling data from the Philadelphia Water Department were examined to explore their potential value for guiding a water utility&amp;rsquo;s progress with regard to optimal corrosion control treatment (OCCT). If a system has established a stable water treatment process with consistent corrosion control treatment (CCT) and has achieved continued decreases in lead levels during regulatory sampling, then the information collected during LCR monitoring can be used as an important data set of a broader OCCT evaluation and will help inform the benefit of additional changes in CCT. Since water utilities have LCR data dating back to 1992, these data should be used to make informed decisions. This research also showed that the addition of orthophosphate has resulted in a significant decrease in lead levels at the customer tap. Additionally, profile sampling was performed to show that first‐draw 1 L samples following a 6 h stagnation period provide a good representation of the lead concentrations measured from lead service line and home plumbing samples at the same sites and may be used to indicate overall changes in lead concentrations at the tap resulting from CCT for this system.&lt;/p&gt;
</description>
  </item>
  
<item>
  <title>Resources</title>
  <link>/resources/</link>
  <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
  
<guid>/resources/</guid>
  <description>

&lt;p&gt;This is a (far from comprehensive) list of resources that I find useful. This list is mostly here to serve as a place to keep references for myself, but maybe others will benefit from it too!&lt;/p&gt;

&lt;h1 id=&#34;data-science-statistics&#34;&gt;Data Science/Statistics&lt;/h1&gt;

&lt;h3 id=&#34;books&#34;&gt;Books&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://r4ds.had.co.nz/&#34;&gt;R for Data Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://adv-r.hadley.nz/&#34;&gt;Advanced R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://r-pkgs.had.co.nz/&#34;&gt;R packages&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;r-packages&#34;&gt;R Packages&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rmarkdown.rstudio.com/articles.html&#34;&gt;&lt;code&gt;rmarkdown&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;&lt;code&gt;shiny&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://deanattali.com/shinyjs/&#34;&gt;&lt;code&gt;shinyjs&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/daattali/shinyalert/&#34;&gt;&lt;code&gt;shinyalert&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tidyverse/reprex&#34;&gt;&lt;code&gt;reprex&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;blogs&#34;&gt;Blogs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.r-bloggers.com/&#34;&gt;R-bloggers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.rstudio.com/&#34;&gt;RStudio blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.business-science.io/blog/index.html&#34;&gt;Business Science&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.fromthebottomoftheheap.net/&#34;&gt;From the bottom of the heap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://varianceexplained.org/&#34;&gt;Variance Explained&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.brodrigues.co/#&#34;&gt;Econometrics and Free Software&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://shirinsplayground.netlify.com/&#34;&gt;Shirin&amp;rsquo;s Playground&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://deanattali.com/&#34;&gt;Dean Attali&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;blog-posts&#34;&gt;Blog Posts&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://eddjberry.netlify.com/post/writing-your-thesis-with-bookdown/&#34;&gt;Writing your thesis with bookdown&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;unix-rstudio-connect&#34;&gt;Unix/RStudio Connect&lt;/h1&gt;

&lt;h3 id=&#34;books-docs&#34;&gt;Books/Docs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://seankross.com/the-unix-workbench/command-line-basics.html&#34;&gt;Unix Workbench&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://docs.rstudio.com/connect/admin/&#34;&gt;RStudio Connect Admin Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://help.ubuntu.com/lts/serverguide/serverguide.pdf&#34;&gt;Ubuntu Server Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://nginx.org/en/docs/beginners_guide.html&#34;&gt;&lt;code&gt;nginx&lt;/code&gt; Beginner&amp;rsquo;s Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;blog-posts-1&#34;&gt;Blog Posts&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-set-up-shiny-server-on-ubuntu-16-04&#34;&gt;How to Set Up Shiny Server on Ubuntu 16.04&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nixtutor.com/freebsd/understanding-symbolic-links/&#34;&gt;Understanding Symbolic Links&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;stack-overflow-https-stackoverflow-com-ask-ubuntu-https-askubuntu-com&#34;&gt;&lt;a href=&#34;https://stackoverflow.com/&#34;&gt;Stack Overflow&lt;/a&gt;/&lt;a href=&#34;https://askubuntu.com/&#34;&gt;Ask Ubuntu&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://askubuntu.com/questions/175172/how-do-i-configure-proxies-without-gui&#34;&gt;Configure Proxies&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;environmental-engineering&#34;&gt;Environmental Engineering&lt;/h1&gt;

&lt;h3 id=&#34;r-packages-1&#34;&gt;R Packages&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/USGS-R/dataRetrieval&#34;&gt;dataRetrieval&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/USGS-R&#34;&gt;USGS-R&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;blogs-1&#34;&gt;Blogs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chaasblog.wordpress.com/&#34;&gt;Charles Haas&amp;rsquo; Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://microbes.cae.drexel.edu/&#34;&gt;The Sales Laboratory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jacobrprice.github.io/&#34;&gt;Jacob Price&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;biology-bioinformatics&#34;&gt;Biology/Bioinformatics&lt;/h1&gt;

&lt;h3 id=&#34;books-1&#34;&gt;Books&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/class/bios221/book/&#34;&gt;Modern Statistics for Modern Biology&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;r-packages-2&#34;&gt;R Packages&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://guangchuangyu.github.io/software/ggtree/&#34;&gt;&lt;code&gt;ggtree&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://guangchuangyu.github.io/software/treeio/&#34;&gt;&lt;code&gt;treeio&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/GuangchuangYu/tidytree&#34;&gt;&lt;code&gt;tidytree&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://benjjneb.github.io/dada2/index.html&#34;&gt;&lt;code&gt;dada2&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://joey711.github.io/phyloseq/&#34;&gt;&lt;code&gt;phyloseq&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mhahsler/rBLAST&#34;&gt;&lt;code&gt;rBLAST&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;blogs-2&#34;&gt;Blogs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.molecularecologist.com/&#34;&gt;The Molecular Ecologist&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
  </item>
  
</channel>
  </rss>